{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "\n",
    "from web_crawler import grabDomainRoot\n",
    "\n",
    "'''\n",
    "randomWalk(graph, initial_node, step, max_step, path)\n",
    "\n",
    "Function to take a random walk from a given node\n",
    "\n",
    "graph: networkx graph, graph from which to random through\n",
    "initial node: string, initial node to begin the walk\n",
    "step: int, current step of walk\n",
    "max_step: int, maximum number of steps to take in walk\n",
    "path:, list, current path taken in the walk\n",
    "'''\n",
    "def randomWalk(graph, initial_node, step, max_step, path):\n",
    " \n",
    "    if step>= max_step: \n",
    "        return path\n",
    "    \n",
    "    adjacent_nodes = [i for i in graph.neighbors(initial_node)]\n",
    "    \n",
    "    next_node = random.sample(adjacent_nodes, 1)[0]\n",
    "    \n",
    "    path.append(next_node)\n",
    "    \n",
    "    return randomWalk(graph, next_node, step+1, max_step, path)\n",
    "\n",
    "'''\n",
    "generateBatch(batch_size, num_context_per_label, context_window, target, step)\n",
    "\n",
    "batch_size: int, batch size for training\n",
    "num_context_per_label: int, how many context examples to use per label (the label is the target) \n",
    "can't be greater than the context window size\n",
    "context_window: int, size of the context window \n",
    "target: array, the list of targets for each context window\n",
    "step: int, counter for how many times to step through the same context and target data\n",
    "\n",
    "Generate the batch data for training. For each \"context window\", randomly sample a\n",
    "set of context elements and configure them as training data by constructing column data of,\n",
    "\n",
    "[target_0, context_0]\n",
    "[target_0, context_1]\n",
    "[target_0, context_3]\n",
    "...\n",
    "[target_n, context_3]\n",
    "[target_n, context_2]\n",
    "[target_n, context_1]\n",
    "\n",
    "'''\n",
    "def generateBatch(batch_size, num_context_per_label, context_window, target, step):\n",
    "\n",
    "    batch = []\n",
    "    passes_through_batch = batch_size//num_context_per_label\n",
    "    \n",
    "    for window_idx in range(passes_through_batch):\n",
    "\n",
    "        current_window = list(context_window[window_idx + passes_through_batch*step])\n",
    "        current_target = target[window_idx + passes_through_batch*step]\n",
    "        context_samples = random.sample(current_window, num_context_per_label)\n",
    "        data_samples =  [[context_sample, [current_target]] for context_sample in context_samples]\n",
    "\n",
    "        for data_sample in data_samples:\n",
    "            batch.append(data_sample)\n",
    "            \n",
    "    return batch\n",
    "\n",
    "black_list = ['@', ':/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = pickle.load(open('crawler_results/graph_calls_620000_stack_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n"
     ]
    }
   ],
   "source": [
    "#Create a graph out of the connections\n",
    "\n",
    "#web_graph = nx.DiGraph()\n",
    "web_graph = nx.Graph()\n",
    "for node in graph_file.keys():\n",
    "    for idx in range(0, len(graph_file[node]), 3):\n",
    "        key = graph_file[node][idx]\n",
    "\n",
    "        domain_node = grabDomainRoot(node)\n",
    "        domain_key = grabDomainRoot(key)\n",
    "        \n",
    "        if domain_node is None or domain_key is None: \n",
    "            continue\n",
    "        if True in [i in domain_node for i in black_list] or True in [i in domain_key for i in black_list]:\n",
    "            continue\n",
    "\n",
    "        web_graph.add_edge(domain_node, domain_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nodes = [i for i in web_graph.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_inv_map = {i:idx for idx, i in enumerate(list_of_nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8943 nodes, 8943 key_domain dict\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "#print('%d nodes, %d dict terms, %d key_domain dict' % (len(list_of_nodes), len(vocab_dict), len(key_domain_dict)))\n",
    "print('%d nodes, %d key_domain dict' % (len(list_of_nodes), len(domain_inv_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = 4# Window size and max_step must be connected\n",
    "\n",
    "num_skips = 2 #The number of context examples per label to create x-y data out of \n",
    "#i.e. the number of rows of \"data\" per window, label combo\n",
    "window_size = max_step//2 #where max step must be even\n",
    "embedding_size = 32  #Dimension of the embedding vector.\n",
    "vocabulary_size = len(web_graph.nodes())\n",
    "\n",
    "num_sampled = 64 #Number of negative examples to sample. \n",
    "#As this number goes to the total number of samples it reproduces softmax, \n",
    "#this not quite correct as we still doing binary classification, except now we give every negative example to test against,\n",
    "#as opposed to true multi-class classification\n",
    "batch_size = 64 #must be a multiple of num_skips\n",
    "num_steps = int(len(list_of_nodes)/batch_size)\n",
    "n_epochs = 10000 #This controls the number of walks from each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8943 nodes, 139 steps per epoch\n"
     ]
    }
   ],
   "source": [
    "print ('%d nodes, %d steps per epoch' % (len(list_of_nodes), num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "epoch:0, Average loss:201.0651\n",
      "epoch:1, Average loss:180.1612\n",
      "epoch:2, Average loss:167.784\n",
      "epoch:3, Average loss:161.2119\n",
      "epoch:4, Average loss:152.6324\n",
      "epoch:5, Average loss:142.3703\n",
      "epoch:6, Average loss:138.4955\n",
      "epoch:7, Average loss:129.1191\n",
      "epoch:8, Average loss:124.9444\n",
      "epoch:9, Average loss:119.9018\n",
      "epoch:10, Average loss:115.8541\n",
      "epoch:11, Average loss:108.5474\n",
      "epoch:12, Average loss:106.3352\n",
      "epoch:13, Average loss:99.02678\n",
      "epoch:14, Average loss:94.69514\n",
      "epoch:15, Average loss:91.58173\n",
      "epoch:16, Average loss:88.32408\n",
      "epoch:17, Average loss:86.52755\n",
      "epoch:18, Average loss:81.67401\n",
      "epoch:19, Average loss:77.82794\n",
      "epoch:20, Average loss:75.2215\n",
      "epoch:21, Average loss:71.08004\n",
      "epoch:22, Average loss:68.647\n",
      "epoch:23, Average loss:66.9712\n",
      "epoch:24, Average loss:64.81722\n",
      "epoch:25, Average loss:62.66742\n",
      "epoch:26, Average loss:59.60324\n",
      "epoch:27, Average loss:58.72591\n",
      "epoch:28, Average loss:54.34379\n",
      "epoch:29, Average loss:54.64056\n",
      "epoch:30, Average loss:50.70596\n",
      "epoch:31, Average loss:50.71321\n",
      "epoch:32, Average loss:49.5081\n",
      "epoch:33, Average loss:47.92974\n",
      "epoch:34, Average loss:46.43287\n",
      "epoch:35, Average loss:43.09906\n",
      "epoch:36, Average loss:43.76121\n",
      "epoch:37, Average loss:41.88367\n",
      "epoch:38, Average loss:40.77678\n",
      "epoch:39, Average loss:41.17244\n",
      "epoch:40, Average loss:39.23832\n",
      "epoch:41, Average loss:37.85771\n",
      "epoch:42, Average loss:37.24628\n",
      "epoch:43, Average loss:36.10667\n",
      "epoch:44, Average loss:36.24379\n",
      "epoch:45, Average loss:33.43665\n",
      "epoch:46, Average loss:33.7134\n",
      "epoch:47, Average loss:32.10096\n",
      "epoch:48, Average loss:30.68781\n",
      "epoch:49, Average loss:31.53707\n",
      "epoch:50, Average loss:30.47503\n",
      "epoch:51, Average loss:29.43881\n",
      "epoch:52, Average loss:29.04997\n",
      "epoch:53, Average loss:27.81308\n",
      "epoch:54, Average loss:28.11223\n",
      "epoch:55, Average loss:26.46206\n",
      "epoch:56, Average loss:25.39162\n",
      "epoch:57, Average loss:25.37864\n",
      "epoch:58, Average loss:25.05699\n",
      "epoch:59, Average loss:24.71055\n",
      "epoch:60, Average loss:23.28006\n",
      "epoch:61, Average loss:22.25153\n",
      "epoch:62, Average loss:22.46254\n",
      "epoch:63, Average loss:22.31252\n",
      "epoch:64, Average loss:21.46831\n",
      "epoch:65, Average loss:21.05315\n",
      "epoch:66, Average loss:20.9117\n",
      "epoch:67, Average loss:19.76123\n",
      "epoch:68, Average loss:19.24379\n",
      "epoch:69, Average loss:19.33585\n",
      "epoch:70, Average loss:18.36549\n",
      "epoch:71, Average loss:18.3841\n",
      "epoch:72, Average loss:17.50914\n",
      "epoch:73, Average loss:17.07367\n",
      "epoch:74, Average loss:17.04745\n",
      "epoch:75, Average loss:16.96153\n",
      "epoch:76, Average loss:16.4365\n",
      "epoch:77, Average loss:16.00695\n",
      "epoch:78, Average loss:15.7971\n",
      "epoch:79, Average loss:15.575\n",
      "epoch:80, Average loss:15.10152\n",
      "epoch:81, Average loss:15.01693\n",
      "epoch:82, Average loss:14.94391\n",
      "epoch:83, Average loss:14.38157\n",
      "epoch:84, Average loss:14.00567\n",
      "epoch:85, Average loss:13.84732\n",
      "epoch:86, Average loss:13.0623\n",
      "epoch:87, Average loss:13.00055\n",
      "epoch:88, Average loss:12.90749\n",
      "epoch:89, Average loss:12.72554\n",
      "epoch:90, Average loss:11.98394\n",
      "epoch:91, Average loss:12.31154\n",
      "epoch:92, Average loss:12.10986\n",
      "epoch:93, Average loss:11.83601\n",
      "epoch:94, Average loss:11.39934\n",
      "epoch:95, Average loss:10.88266\n",
      "epoch:96, Average loss:11.33726\n",
      "epoch:97, Average loss:10.95049\n",
      "epoch:98, Average loss:10.3342\n",
      "epoch:99, Average loss:10.20088\n",
      "epoch:100, Average loss:10.38124\n",
      "epoch:101, Average loss:10.11704\n",
      "epoch:102, Average loss:9.73208\n",
      "epoch:103, Average loss:9.407442\n",
      "epoch:104, Average loss:9.437505\n",
      "epoch:105, Average loss:9.356387\n",
      "epoch:106, Average loss:9.105576\n",
      "epoch:107, Average loss:8.832881\n",
      "epoch:108, Average loss:8.918378\n",
      "epoch:109, Average loss:8.722185\n",
      "epoch:110, Average loss:8.609481\n",
      "epoch:111, Average loss:8.542295\n",
      "epoch:112, Average loss:8.155122\n",
      "epoch:113, Average loss:8.118804\n",
      "epoch:114, Average loss:8.116036\n",
      "epoch:115, Average loss:7.717801\n",
      "epoch:116, Average loss:7.651708\n",
      "epoch:117, Average loss:7.716107\n",
      "epoch:118, Average loss:7.762791\n",
      "epoch:119, Average loss:7.305063\n",
      "epoch:120, Average loss:7.559858\n",
      "epoch:121, Average loss:7.331591\n",
      "epoch:122, Average loss:7.263459\n",
      "epoch:123, Average loss:7.03832\n",
      "epoch:124, Average loss:6.989606\n",
      "epoch:125, Average loss:6.691471\n",
      "epoch:126, Average loss:6.864502\n",
      "epoch:127, Average loss:6.816136\n",
      "epoch:128, Average loss:6.452119\n",
      "epoch:129, Average loss:6.560413\n",
      "epoch:130, Average loss:6.497292\n",
      "epoch:131, Average loss:6.347576\n",
      "epoch:132, Average loss:6.241972\n",
      "epoch:133, Average loss:6.252567\n",
      "epoch:134, Average loss:6.135785\n",
      "epoch:135, Average loss:6.07788\n",
      "epoch:136, Average loss:6.053977\n",
      "epoch:137, Average loss:5.900355\n",
      "epoch:138, Average loss:6.071346\n",
      "epoch:139, Average loss:5.769924\n",
      "epoch:140, Average loss:5.644351\n",
      "epoch:141, Average loss:5.722909\n",
      "epoch:142, Average loss:5.499218\n",
      "epoch:143, Average loss:5.756057\n",
      "epoch:144, Average loss:5.611436\n",
      "epoch:145, Average loss:5.500435\n",
      "epoch:146, Average loss:5.403626\n",
      "epoch:147, Average loss:5.440416\n",
      "epoch:148, Average loss:5.478868\n",
      "epoch:149, Average loss:5.288534\n",
      "epoch:150, Average loss:5.172744\n",
      "epoch:151, Average loss:5.223229\n",
      "epoch:152, Average loss:5.164067\n",
      "epoch:153, Average loss:5.201249\n",
      "epoch:154, Average loss:5.115931\n",
      "epoch:155, Average loss:5.141007\n",
      "epoch:156, Average loss:4.922977\n",
      "epoch:157, Average loss:5.019083\n",
      "epoch:158, Average loss:4.988473\n",
      "epoch:159, Average loss:4.936355\n",
      "epoch:160, Average loss:4.950652\n",
      "epoch:161, Average loss:4.726018\n",
      "epoch:162, Average loss:4.77053\n",
      "epoch:163, Average loss:4.807525\n",
      "epoch:164, Average loss:4.753605\n",
      "epoch:165, Average loss:4.659281\n",
      "epoch:166, Average loss:4.634065\n",
      "epoch:167, Average loss:4.661822\n",
      "epoch:168, Average loss:4.569324\n",
      "epoch:169, Average loss:4.53392\n",
      "epoch:170, Average loss:4.539425\n",
      "epoch:171, Average loss:4.555039\n",
      "epoch:172, Average loss:4.515184\n",
      "epoch:173, Average loss:4.384088\n",
      "epoch:174, Average loss:4.490901\n",
      "epoch:175, Average loss:4.446208\n",
      "epoch:176, Average loss:4.409692\n",
      "epoch:177, Average loss:4.39345\n",
      "epoch:178, Average loss:4.348464\n",
      "epoch:179, Average loss:4.335888\n",
      "epoch:180, Average loss:4.343469\n",
      "epoch:181, Average loss:4.298602\n",
      "epoch:182, Average loss:4.350706\n",
      "epoch:183, Average loss:4.308999\n",
      "epoch:184, Average loss:4.228463\n",
      "epoch:185, Average loss:4.309522\n",
      "epoch:186, Average loss:4.267646\n",
      "epoch:187, Average loss:4.225572\n",
      "epoch:188, Average loss:4.240403\n",
      "epoch:189, Average loss:4.051926\n",
      "epoch:190, Average loss:4.115166\n",
      "epoch:191, Average loss:4.121716\n",
      "epoch:192, Average loss:4.050109\n",
      "epoch:193, Average loss:4.153056\n",
      "epoch:194, Average loss:4.139537\n",
      "epoch:195, Average loss:4.082554\n",
      "epoch:196, Average loss:4.024042\n",
      "epoch:197, Average loss:4.03789\n",
      "epoch:198, Average loss:4.084983\n",
      "epoch:199, Average loss:4.022283\n",
      "epoch:200, Average loss:3.915237\n",
      "epoch:201, Average loss:4.006614\n",
      "epoch:202, Average loss:3.943399\n",
      "epoch:203, Average loss:3.969064\n",
      "epoch:204, Average loss:3.918286\n",
      "epoch:205, Average loss:4.072715\n",
      "epoch:206, Average loss:3.950033\n",
      "epoch:207, Average loss:3.876711\n",
      "epoch:208, Average loss:3.940319\n",
      "epoch:209, Average loss:3.865631\n",
      "epoch:210, Average loss:3.914841\n",
      "epoch:211, Average loss:3.861768\n",
      "epoch:212, Average loss:3.864606\n",
      "epoch:213, Average loss:3.894479\n",
      "epoch:214, Average loss:3.818132\n",
      "epoch:215, Average loss:3.844365\n",
      "epoch:216, Average loss:3.889924\n",
      "epoch:217, Average loss:3.82152\n",
      "epoch:218, Average loss:3.750142\n",
      "epoch:219, Average loss:3.852371\n",
      "epoch:220, Average loss:3.804798\n",
      "epoch:221, Average loss:3.765716\n",
      "epoch:222, Average loss:3.843334\n",
      "epoch:223, Average loss:3.746066\n",
      "epoch:224, Average loss:3.749154\n",
      "epoch:225, Average loss:3.73687\n",
      "epoch:226, Average loss:3.715301\n",
      "epoch:227, Average loss:3.785915\n",
      "epoch:228, Average loss:3.702468\n",
      "epoch:229, Average loss:3.711\n",
      "epoch:230, Average loss:3.758248\n",
      "epoch:231, Average loss:3.717731\n",
      "epoch:232, Average loss:3.695058\n",
      "epoch:233, Average loss:3.705455\n",
      "epoch:234, Average loss:3.690593\n",
      "epoch:235, Average loss:3.736004\n",
      "epoch:236, Average loss:3.742233\n",
      "epoch:237, Average loss:3.630962\n",
      "epoch:238, Average loss:3.728902\n",
      "epoch:239, Average loss:3.614994\n",
      "epoch:240, Average loss:3.660359\n",
      "epoch:241, Average loss:3.661323\n",
      "epoch:242, Average loss:3.678752\n",
      "epoch:243, Average loss:3.618584\n",
      "epoch:244, Average loss:3.595957\n",
      "epoch:245, Average loss:3.656393\n",
      "epoch:246, Average loss:3.582004\n",
      "epoch:247, Average loss:3.585082\n",
      "epoch:248, Average loss:3.578777\n",
      "epoch:249, Average loss:3.60887\n",
      "epoch:250, Average loss:3.544529\n",
      "epoch:251, Average loss:3.508821\n",
      "epoch:252, Average loss:3.598564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:253, Average loss:3.536277\n",
      "epoch:254, Average loss:3.611324\n",
      "epoch:255, Average loss:3.625783\n",
      "epoch:256, Average loss:3.520257\n",
      "epoch:257, Average loss:3.634608\n",
      "epoch:258, Average loss:3.579152\n",
      "epoch:259, Average loss:3.566695\n",
      "epoch:260, Average loss:3.531519\n",
      "epoch:261, Average loss:3.497372\n",
      "epoch:262, Average loss:3.523329\n",
      "epoch:263, Average loss:3.452744\n",
      "epoch:264, Average loss:3.496967\n",
      "epoch:265, Average loss:3.485909\n",
      "epoch:266, Average loss:3.505375\n",
      "epoch:267, Average loss:3.49231\n",
      "epoch:268, Average loss:3.567245\n",
      "epoch:269, Average loss:3.45723\n",
      "epoch:270, Average loss:3.503774\n",
      "epoch:271, Average loss:3.420156\n",
      "epoch:272, Average loss:3.492\n",
      "epoch:273, Average loss:3.461158\n",
      "epoch:274, Average loss:3.519635\n",
      "epoch:275, Average loss:3.473402\n",
      "epoch:276, Average loss:3.456997\n",
      "epoch:277, Average loss:3.413737\n",
      "epoch:278, Average loss:3.47703\n",
      "epoch:279, Average loss:3.443362\n",
      "epoch:280, Average loss:3.42652\n",
      "epoch:281, Average loss:3.484167\n",
      "epoch:282, Average loss:3.458219\n",
      "epoch:283, Average loss:3.407167\n",
      "epoch:284, Average loss:3.388821\n",
      "epoch:285, Average loss:3.445178\n",
      "epoch:286, Average loss:3.344531\n",
      "epoch:287, Average loss:3.384617\n",
      "epoch:288, Average loss:3.486928\n",
      "epoch:289, Average loss:3.366189\n",
      "epoch:290, Average loss:3.369518\n",
      "epoch:291, Average loss:3.428343\n",
      "epoch:292, Average loss:3.374103\n",
      "epoch:293, Average loss:3.352179\n",
      "epoch:294, Average loss:3.357285\n",
      "epoch:295, Average loss:3.406256\n",
      "epoch:296, Average loss:3.357425\n",
      "epoch:297, Average loss:3.382685\n",
      "epoch:298, Average loss:3.329738\n",
      "epoch:299, Average loss:3.360991\n",
      "epoch:300, Average loss:3.364358\n",
      "epoch:301, Average loss:3.350125\n",
      "epoch:302, Average loss:3.32481\n",
      "epoch:303, Average loss:3.337262\n",
      "epoch:304, Average loss:3.414717\n",
      "epoch:305, Average loss:3.381495\n",
      "epoch:306, Average loss:3.313104\n",
      "epoch:307, Average loss:3.272869\n",
      "epoch:308, Average loss:3.290284\n",
      "epoch:309, Average loss:3.325552\n",
      "epoch:310, Average loss:3.313572\n",
      "epoch:311, Average loss:3.306492\n",
      "epoch:312, Average loss:3.265221\n",
      "epoch:313, Average loss:3.342609\n",
      "epoch:314, Average loss:3.322856\n",
      "epoch:315, Average loss:3.267287\n",
      "epoch:316, Average loss:3.29689\n",
      "epoch:317, Average loss:3.277843\n",
      "epoch:318, Average loss:3.297327\n",
      "epoch:319, Average loss:3.274695\n",
      "epoch:320, Average loss:3.362416\n",
      "epoch:321, Average loss:3.27156\n",
      "epoch:322, Average loss:3.301812\n",
      "epoch:323, Average loss:3.193748\n",
      "epoch:324, Average loss:3.255576\n",
      "epoch:325, Average loss:3.284609\n",
      "epoch:326, Average loss:3.276352\n",
      "epoch:327, Average loss:3.284\n",
      "epoch:328, Average loss:3.200723\n",
      "epoch:329, Average loss:3.240364\n",
      "epoch:330, Average loss:3.255396\n",
      "epoch:331, Average loss:3.235069\n",
      "epoch:332, Average loss:3.199721\n",
      "epoch:333, Average loss:3.273792\n",
      "epoch:334, Average loss:3.227881\n",
      "epoch:335, Average loss:3.308973\n",
      "epoch:336, Average loss:3.276165\n",
      "epoch:337, Average loss:3.241988\n",
      "epoch:338, Average loss:3.231083\n",
      "epoch:339, Average loss:3.221226\n",
      "epoch:340, Average loss:3.251933\n",
      "epoch:341, Average loss:3.161686\n",
      "epoch:342, Average loss:3.240436\n",
      "epoch:343, Average loss:3.189991\n",
      "epoch:344, Average loss:3.185319\n",
      "epoch:345, Average loss:3.168538\n",
      "epoch:346, Average loss:3.25183\n",
      "epoch:347, Average loss:3.181069\n",
      "epoch:348, Average loss:3.232777\n",
      "epoch:349, Average loss:3.245797\n",
      "epoch:350, Average loss:3.186255\n",
      "epoch:351, Average loss:3.239178\n",
      "epoch:352, Average loss:3.208261\n",
      "epoch:353, Average loss:3.164011\n",
      "epoch:354, Average loss:3.221466\n",
      "epoch:355, Average loss:3.127037\n",
      "epoch:356, Average loss:3.208605\n",
      "epoch:357, Average loss:3.214411\n",
      "epoch:358, Average loss:3.180392\n",
      "epoch:359, Average loss:3.237889\n",
      "epoch:360, Average loss:3.127423\n",
      "epoch:361, Average loss:3.231563\n",
      "epoch:362, Average loss:3.201662\n",
      "epoch:363, Average loss:3.140956\n",
      "epoch:364, Average loss:3.149911\n",
      "epoch:365, Average loss:3.17635\n",
      "epoch:366, Average loss:3.150877\n",
      "epoch:367, Average loss:3.134097\n",
      "epoch:368, Average loss:3.106664\n",
      "epoch:369, Average loss:3.082955\n",
      "epoch:370, Average loss:3.166663\n",
      "epoch:371, Average loss:3.142399\n",
      "epoch:372, Average loss:3.181345\n",
      "epoch:373, Average loss:3.212625\n",
      "epoch:374, Average loss:3.111707\n",
      "epoch:375, Average loss:3.213044\n",
      "epoch:376, Average loss:3.175622\n",
      "epoch:377, Average loss:3.17501\n",
      "epoch:378, Average loss:3.145076\n",
      "epoch:379, Average loss:3.11199\n",
      "epoch:380, Average loss:3.137666\n",
      "epoch:381, Average loss:3.127091\n",
      "epoch:382, Average loss:3.171613\n",
      "epoch:383, Average loss:3.134981\n",
      "epoch:384, Average loss:3.118215\n",
      "epoch:385, Average loss:3.197299\n",
      "epoch:386, Average loss:3.081019\n",
      "epoch:387, Average loss:3.112984\n",
      "epoch:388, Average loss:3.102415\n",
      "epoch:389, Average loss:3.051209\n",
      "epoch:390, Average loss:3.12379\n",
      "epoch:391, Average loss:3.144125\n",
      "epoch:392, Average loss:3.068328\n",
      "epoch:393, Average loss:3.109061\n",
      "epoch:394, Average loss:3.113458\n",
      "epoch:395, Average loss:3.119666\n",
      "epoch:396, Average loss:3.171471\n",
      "epoch:397, Average loss:3.10115\n",
      "epoch:398, Average loss:3.151076\n",
      "epoch:399, Average loss:3.131943\n",
      "epoch:400, Average loss:3.101527\n",
      "epoch:401, Average loss:3.011571\n",
      "epoch:402, Average loss:3.079656\n",
      "epoch:403, Average loss:3.11691\n",
      "epoch:404, Average loss:3.02618\n",
      "epoch:405, Average loss:3.077609\n",
      "epoch:406, Average loss:3.074096\n",
      "epoch:407, Average loss:3.091228\n",
      "epoch:408, Average loss:3.04348\n",
      "epoch:409, Average loss:3.056682\n",
      "epoch:410, Average loss:3.064404\n",
      "epoch:411, Average loss:3.02642\n",
      "epoch:412, Average loss:3.058092\n",
      "epoch:413, Average loss:3.046422\n",
      "epoch:414, Average loss:3.040249\n",
      "epoch:415, Average loss:3.132186\n",
      "epoch:416, Average loss:2.987027\n",
      "epoch:417, Average loss:3.063686\n",
      "epoch:418, Average loss:3.05494\n",
      "epoch:419, Average loss:3.069288\n",
      "epoch:420, Average loss:3.140663\n",
      "epoch:421, Average loss:3.10729\n",
      "epoch:422, Average loss:3.043978\n",
      "epoch:423, Average loss:3.050773\n",
      "epoch:424, Average loss:3.01799\n",
      "epoch:425, Average loss:3.041568\n",
      "epoch:426, Average loss:3.047446\n",
      "epoch:427, Average loss:3.051318\n",
      "epoch:428, Average loss:3.061048\n",
      "epoch:429, Average loss:2.985158\n",
      "epoch:430, Average loss:3.112687\n",
      "epoch:431, Average loss:3.075061\n",
      "epoch:432, Average loss:3.115646\n",
      "epoch:433, Average loss:3.048289\n",
      "epoch:434, Average loss:3.040909\n",
      "epoch:435, Average loss:3.049965\n",
      "epoch:436, Average loss:3.06243\n",
      "epoch:437, Average loss:3.058552\n",
      "epoch:438, Average loss:3.090698\n",
      "epoch:439, Average loss:3.050046\n",
      "epoch:440, Average loss:3.071201\n",
      "epoch:441, Average loss:3.030808\n",
      "epoch:442, Average loss:2.992877\n",
      "epoch:443, Average loss:3.038927\n",
      "epoch:444, Average loss:3.018977\n",
      "epoch:445, Average loss:3.018178\n",
      "epoch:446, Average loss:2.983799\n",
      "epoch:447, Average loss:3.045381\n",
      "epoch:448, Average loss:3.011571\n",
      "epoch:449, Average loss:2.978991\n",
      "epoch:450, Average loss:3.039536\n",
      "epoch:451, Average loss:3.047692\n",
      "epoch:452, Average loss:2.96346\n",
      "epoch:453, Average loss:3.008914\n",
      "epoch:454, Average loss:3.028269\n",
      "epoch:455, Average loss:2.934949\n",
      "epoch:456, Average loss:3.001388\n",
      "epoch:457, Average loss:2.977409\n",
      "epoch:458, Average loss:3.046667\n",
      "epoch:459, Average loss:3.046292\n",
      "epoch:460, Average loss:3.038292\n",
      "epoch:461, Average loss:3.077897\n",
      "epoch:462, Average loss:3.026715\n",
      "epoch:463, Average loss:2.975233\n",
      "epoch:464, Average loss:2.978272\n",
      "epoch:465, Average loss:3.007626\n",
      "epoch:466, Average loss:3.009156\n",
      "epoch:467, Average loss:3.005333\n",
      "epoch:468, Average loss:2.962449\n",
      "epoch:469, Average loss:2.969759\n",
      "epoch:470, Average loss:2.997263\n",
      "epoch:471, Average loss:3.035711\n",
      "epoch:472, Average loss:2.97193\n",
      "epoch:473, Average loss:2.994067\n",
      "epoch:474, Average loss:3.020313\n",
      "epoch:475, Average loss:2.957415\n",
      "epoch:476, Average loss:2.943696\n",
      "epoch:477, Average loss:2.977229\n",
      "epoch:478, Average loss:2.981914\n",
      "epoch:479, Average loss:2.945964\n",
      "epoch:480, Average loss:2.986139\n",
      "epoch:481, Average loss:3.009021\n",
      "epoch:482, Average loss:2.964547\n",
      "epoch:483, Average loss:2.972499\n",
      "epoch:484, Average loss:2.934929\n",
      "epoch:485, Average loss:2.991594\n",
      "epoch:486, Average loss:2.979274\n",
      "epoch:487, Average loss:2.984972\n",
      "epoch:488, Average loss:2.948818\n",
      "epoch:489, Average loss:2.983513\n",
      "epoch:490, Average loss:2.998831\n",
      "epoch:491, Average loss:2.960912\n",
      "epoch:492, Average loss:2.948219\n",
      "epoch:493, Average loss:2.965115\n",
      "epoch:494, Average loss:2.934999\n",
      "epoch:495, Average loss:2.917485\n",
      "epoch:496, Average loss:2.949967\n",
      "epoch:497, Average loss:2.918906\n",
      "epoch:498, Average loss:2.986207\n",
      "epoch:499, Average loss:2.938279\n",
      "epoch:500, Average loss:2.944871\n",
      "epoch:501, Average loss:2.952551\n",
      "epoch:502, Average loss:2.930201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:503, Average loss:2.969147\n",
      "epoch:504, Average loss:2.92428\n",
      "epoch:505, Average loss:2.936592\n",
      "epoch:506, Average loss:2.977297\n",
      "epoch:507, Average loss:2.95936\n",
      "epoch:508, Average loss:2.978122\n",
      "epoch:509, Average loss:2.908093\n",
      "epoch:510, Average loss:2.932312\n",
      "epoch:511, Average loss:2.939285\n",
      "epoch:512, Average loss:2.964261\n",
      "epoch:513, Average loss:2.876209\n",
      "epoch:514, Average loss:2.9291\n",
      "epoch:515, Average loss:2.943052\n",
      "epoch:516, Average loss:2.922459\n",
      "epoch:517, Average loss:2.903786\n",
      "epoch:518, Average loss:2.937923\n",
      "epoch:519, Average loss:2.951805\n",
      "epoch:520, Average loss:2.875827\n",
      "epoch:521, Average loss:2.936041\n",
      "epoch:522, Average loss:2.877187\n",
      "epoch:523, Average loss:2.87695\n",
      "epoch:524, Average loss:2.861285\n",
      "epoch:525, Average loss:2.905636\n",
      "epoch:526, Average loss:2.916723\n",
      "epoch:527, Average loss:2.902006\n",
      "epoch:528, Average loss:2.920549\n",
      "epoch:529, Average loss:2.899446\n",
      "epoch:530, Average loss:2.870859\n",
      "epoch:531, Average loss:2.899411\n",
      "epoch:532, Average loss:2.935432\n",
      "epoch:533, Average loss:2.89778\n",
      "epoch:534, Average loss:2.875364\n",
      "epoch:535, Average loss:2.875912\n",
      "epoch:536, Average loss:2.956436\n",
      "epoch:537, Average loss:2.981231\n",
      "epoch:538, Average loss:2.837244\n",
      "epoch:539, Average loss:2.858027\n",
      "epoch:540, Average loss:2.909982\n",
      "epoch:541, Average loss:2.828003\n",
      "epoch:542, Average loss:2.816069\n",
      "epoch:543, Average loss:2.886546\n",
      "epoch:544, Average loss:2.909952\n",
      "epoch:545, Average loss:2.893724\n",
      "epoch:546, Average loss:2.869121\n",
      "epoch:547, Average loss:2.859199\n",
      "epoch:548, Average loss:2.891203\n",
      "epoch:549, Average loss:2.835623\n",
      "epoch:550, Average loss:2.882941\n",
      "epoch:551, Average loss:2.941724\n",
      "epoch:552, Average loss:2.83087\n",
      "epoch:553, Average loss:2.852137\n",
      "epoch:554, Average loss:2.854728\n",
      "epoch:555, Average loss:2.830224\n",
      "epoch:556, Average loss:2.839307\n",
      "epoch:557, Average loss:2.823302\n",
      "epoch:558, Average loss:2.962077\n",
      "epoch:559, Average loss:2.859216\n",
      "epoch:560, Average loss:2.822494\n",
      "epoch:561, Average loss:2.885863\n",
      "epoch:562, Average loss:2.854559\n",
      "epoch:563, Average loss:2.834667\n",
      "epoch:564, Average loss:2.905984\n",
      "epoch:565, Average loss:2.840609\n",
      "epoch:566, Average loss:2.861027\n",
      "epoch:567, Average loss:2.817872\n",
      "epoch:568, Average loss:2.896078\n",
      "epoch:569, Average loss:2.818126\n",
      "epoch:570, Average loss:2.916138\n",
      "epoch:571, Average loss:2.86127\n",
      "epoch:572, Average loss:2.896246\n",
      "epoch:573, Average loss:2.859121\n",
      "epoch:574, Average loss:2.839109\n",
      "epoch:575, Average loss:2.841479\n",
      "epoch:576, Average loss:2.853937\n",
      "epoch:577, Average loss:2.85562\n",
      "epoch:578, Average loss:2.851535\n",
      "epoch:579, Average loss:2.825355\n",
      "epoch:580, Average loss:2.788624\n",
      "epoch:581, Average loss:2.866081\n",
      "epoch:582, Average loss:2.898872\n",
      "epoch:583, Average loss:2.852636\n",
      "epoch:584, Average loss:2.820128\n",
      "epoch:585, Average loss:2.804941\n",
      "epoch:586, Average loss:2.875109\n",
      "epoch:587, Average loss:2.900179\n",
      "epoch:588, Average loss:2.839557\n",
      "epoch:589, Average loss:2.815548\n",
      "epoch:590, Average loss:2.787487\n",
      "epoch:591, Average loss:2.755856\n",
      "epoch:592, Average loss:2.863518\n",
      "epoch:593, Average loss:2.86128\n",
      "epoch:594, Average loss:2.732703\n",
      "epoch:595, Average loss:2.87111\n",
      "epoch:596, Average loss:2.880903\n",
      "epoch:597, Average loss:2.814625\n",
      "epoch:598, Average loss:2.869318\n",
      "epoch:599, Average loss:2.86425\n",
      "epoch:600, Average loss:2.841451\n",
      "epoch:601, Average loss:2.818038\n",
      "epoch:602, Average loss:2.852329\n",
      "epoch:603, Average loss:2.835214\n",
      "epoch:604, Average loss:2.865736\n",
      "epoch:605, Average loss:2.832643\n",
      "epoch:606, Average loss:2.842552\n",
      "epoch:607, Average loss:2.788028\n",
      "epoch:608, Average loss:2.791819\n",
      "epoch:609, Average loss:2.792958\n",
      "epoch:610, Average loss:2.772348\n",
      "epoch:611, Average loss:2.797965\n",
      "epoch:612, Average loss:2.862278\n",
      "epoch:613, Average loss:2.788216\n",
      "epoch:614, Average loss:2.798338\n",
      "epoch:615, Average loss:2.7932\n",
      "epoch:616, Average loss:2.834606\n",
      "epoch:617, Average loss:2.771288\n",
      "epoch:618, Average loss:2.80647\n",
      "epoch:619, Average loss:2.834041\n",
      "epoch:620, Average loss:2.802802\n",
      "epoch:621, Average loss:2.798954\n",
      "epoch:622, Average loss:2.74179\n",
      "epoch:623, Average loss:2.793517\n",
      "epoch:624, Average loss:2.796143\n",
      "epoch:625, Average loss:2.776129\n",
      "epoch:626, Average loss:2.811922\n",
      "epoch:627, Average loss:2.766109\n",
      "epoch:628, Average loss:2.756776\n",
      "epoch:629, Average loss:2.82895\n",
      "epoch:630, Average loss:2.767837\n",
      "epoch:631, Average loss:2.796338\n",
      "epoch:632, Average loss:2.789714\n",
      "epoch:633, Average loss:2.762904\n",
      "epoch:634, Average loss:2.809\n",
      "epoch:635, Average loss:2.716758\n",
      "epoch:636, Average loss:2.854408\n",
      "epoch:637, Average loss:2.816993\n",
      "epoch:638, Average loss:2.759105\n",
      "epoch:639, Average loss:2.775357\n",
      "epoch:640, Average loss:2.738757\n",
      "epoch:641, Average loss:2.775309\n",
      "epoch:642, Average loss:2.727057\n",
      "epoch:643, Average loss:2.788503\n",
      "epoch:644, Average loss:2.735275\n",
      "epoch:645, Average loss:2.795408\n",
      "epoch:646, Average loss:2.715427\n",
      "epoch:647, Average loss:2.738304\n",
      "epoch:648, Average loss:2.758873\n",
      "epoch:649, Average loss:2.749785\n",
      "epoch:650, Average loss:2.802855\n",
      "epoch:651, Average loss:2.805038\n",
      "epoch:652, Average loss:2.790889\n",
      "epoch:653, Average loss:2.856515\n",
      "epoch:654, Average loss:2.819637\n",
      "epoch:655, Average loss:2.780338\n",
      "epoch:656, Average loss:2.74261\n",
      "epoch:657, Average loss:2.773433\n",
      "epoch:658, Average loss:2.739132\n",
      "epoch:659, Average loss:2.831384\n",
      "epoch:660, Average loss:2.79739\n",
      "epoch:661, Average loss:2.746163\n",
      "epoch:662, Average loss:2.715182\n",
      "epoch:663, Average loss:2.764456\n",
      "epoch:664, Average loss:2.732565\n",
      "epoch:665, Average loss:2.782422\n",
      "epoch:666, Average loss:2.709237\n",
      "epoch:667, Average loss:2.713433\n",
      "epoch:668, Average loss:2.846155\n",
      "epoch:669, Average loss:2.710747\n",
      "epoch:670, Average loss:2.668223\n",
      "epoch:671, Average loss:2.813648\n",
      "epoch:672, Average loss:2.752386\n",
      "epoch:673, Average loss:2.792295\n",
      "epoch:674, Average loss:2.688215\n",
      "epoch:675, Average loss:2.679232\n",
      "epoch:676, Average loss:2.738794\n",
      "epoch:677, Average loss:2.724259\n",
      "epoch:678, Average loss:2.715785\n",
      "epoch:679, Average loss:2.750009\n",
      "epoch:680, Average loss:2.758456\n",
      "epoch:681, Average loss:2.754371\n",
      "epoch:682, Average loss:2.693262\n",
      "epoch:683, Average loss:2.701773\n",
      "epoch:684, Average loss:2.71381\n",
      "epoch:685, Average loss:2.66142\n",
      "epoch:686, Average loss:2.830396\n",
      "epoch:687, Average loss:2.744942\n",
      "epoch:688, Average loss:2.671172\n",
      "epoch:689, Average loss:2.699849\n",
      "epoch:690, Average loss:2.698272\n",
      "epoch:691, Average loss:2.701363\n",
      "epoch:692, Average loss:2.785774\n",
      "epoch:693, Average loss:2.723237\n",
      "epoch:694, Average loss:2.783543\n",
      "epoch:695, Average loss:2.7852\n",
      "epoch:696, Average loss:2.771507\n",
      "epoch:697, Average loss:2.685937\n",
      "epoch:698, Average loss:2.761894\n",
      "epoch:699, Average loss:2.739599\n",
      "epoch:700, Average loss:2.716326\n",
      "epoch:701, Average loss:2.721891\n",
      "epoch:702, Average loss:2.714948\n",
      "epoch:703, Average loss:2.70077\n",
      "epoch:704, Average loss:2.714533\n",
      "epoch:705, Average loss:2.767497\n",
      "epoch:706, Average loss:2.739442\n",
      "epoch:707, Average loss:2.742686\n",
      "epoch:708, Average loss:2.654773\n",
      "epoch:709, Average loss:2.732979\n",
      "epoch:710, Average loss:2.742874\n",
      "epoch:711, Average loss:2.747905\n",
      "epoch:712, Average loss:2.76562\n",
      "epoch:713, Average loss:2.669718\n",
      "epoch:714, Average loss:2.71193\n",
      "epoch:715, Average loss:2.667271\n",
      "epoch:716, Average loss:2.662351\n",
      "epoch:717, Average loss:2.774565\n",
      "epoch:718, Average loss:2.654605\n",
      "epoch:719, Average loss:2.709397\n",
      "epoch:720, Average loss:2.688487\n",
      "epoch:721, Average loss:2.702305\n",
      "epoch:722, Average loss:2.681145\n",
      "epoch:723, Average loss:2.742171\n",
      "epoch:724, Average loss:2.711363\n",
      "epoch:725, Average loss:2.717381\n",
      "epoch:726, Average loss:2.728486\n",
      "epoch:727, Average loss:2.670262\n",
      "epoch:728, Average loss:2.712868\n",
      "epoch:729, Average loss:2.72063\n",
      "epoch:730, Average loss:2.676951\n",
      "epoch:731, Average loss:2.68075\n",
      "epoch:732, Average loss:2.679855\n",
      "epoch:733, Average loss:2.685227\n",
      "epoch:734, Average loss:2.74296\n",
      "epoch:735, Average loss:2.698376\n",
      "epoch:736, Average loss:2.696152\n",
      "epoch:737, Average loss:2.701648\n",
      "epoch:738, Average loss:2.667356\n",
      "epoch:739, Average loss:2.670744\n",
      "epoch:740, Average loss:2.637748\n",
      "epoch:741, Average loss:2.671759\n",
      "epoch:742, Average loss:2.657803\n",
      "epoch:743, Average loss:2.725961\n",
      "epoch:744, Average loss:2.704749\n",
      "epoch:745, Average loss:2.670519\n",
      "epoch:746, Average loss:2.693932\n",
      "epoch:747, Average loss:2.69861\n",
      "epoch:748, Average loss:2.663041\n",
      "epoch:749, Average loss:2.709816\n",
      "epoch:750, Average loss:2.712201\n",
      "epoch:751, Average loss:2.684105\n",
      "epoch:752, Average loss:2.674683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:753, Average loss:2.705233\n",
      "epoch:754, Average loss:2.656785\n",
      "epoch:755, Average loss:2.7173\n",
      "epoch:756, Average loss:2.66298\n",
      "epoch:757, Average loss:2.665103\n",
      "epoch:758, Average loss:2.641633\n",
      "epoch:759, Average loss:2.692214\n",
      "epoch:760, Average loss:2.710087\n",
      "epoch:761, Average loss:2.661892\n",
      "epoch:762, Average loss:2.715034\n",
      "epoch:763, Average loss:2.635388\n",
      "epoch:764, Average loss:2.698218\n",
      "epoch:765, Average loss:2.68229\n",
      "epoch:766, Average loss:2.678867\n",
      "epoch:767, Average loss:2.667302\n",
      "epoch:768, Average loss:2.632492\n",
      "epoch:769, Average loss:2.652424\n",
      "epoch:770, Average loss:2.61101\n",
      "epoch:771, Average loss:2.618566\n",
      "epoch:772, Average loss:2.693602\n",
      "epoch:773, Average loss:2.69277\n",
      "epoch:774, Average loss:2.655165\n",
      "epoch:775, Average loss:2.693408\n",
      "epoch:776, Average loss:2.6534\n",
      "epoch:777, Average loss:2.696608\n",
      "epoch:778, Average loss:2.671868\n",
      "epoch:779, Average loss:2.65923\n",
      "epoch:780, Average loss:2.70295\n",
      "epoch:781, Average loss:2.669515\n",
      "epoch:782, Average loss:2.632166\n",
      "epoch:783, Average loss:2.66698\n",
      "epoch:784, Average loss:2.642086\n",
      "epoch:785, Average loss:2.707409\n",
      "epoch:786, Average loss:2.678982\n",
      "epoch:787, Average loss:2.644338\n",
      "epoch:788, Average loss:2.651861\n",
      "epoch:789, Average loss:2.644165\n",
      "epoch:790, Average loss:2.604829\n",
      "epoch:791, Average loss:2.667306\n",
      "epoch:792, Average loss:2.649435\n",
      "epoch:793, Average loss:2.632158\n",
      "epoch:794, Average loss:2.652405\n",
      "epoch:795, Average loss:2.650812\n",
      "epoch:796, Average loss:2.598672\n",
      "epoch:797, Average loss:2.668678\n",
      "epoch:798, Average loss:2.602991\n",
      "epoch:799, Average loss:2.639044\n",
      "epoch:800, Average loss:2.692038\n",
      "epoch:801, Average loss:2.637726\n",
      "epoch:802, Average loss:2.655762\n",
      "epoch:803, Average loss:2.633021\n",
      "epoch:804, Average loss:2.631406\n",
      "epoch:805, Average loss:2.591744\n",
      "epoch:806, Average loss:2.640387\n",
      "epoch:807, Average loss:2.630817\n",
      "epoch:808, Average loss:2.613913\n",
      "epoch:809, Average loss:2.672147\n",
      "epoch:810, Average loss:2.598356\n",
      "epoch:811, Average loss:2.643003\n",
      "epoch:812, Average loss:2.654109\n",
      "epoch:813, Average loss:2.601044\n",
      "epoch:814, Average loss:2.624244\n",
      "epoch:815, Average loss:2.629551\n",
      "epoch:816, Average loss:2.563536\n",
      "epoch:817, Average loss:2.628747\n",
      "epoch:818, Average loss:2.57688\n",
      "epoch:819, Average loss:2.629404\n",
      "epoch:820, Average loss:2.573299\n",
      "epoch:821, Average loss:2.651478\n",
      "epoch:822, Average loss:2.624687\n",
      "epoch:823, Average loss:2.620989\n",
      "epoch:824, Average loss:2.597056\n",
      "epoch:825, Average loss:2.670686\n",
      "epoch:826, Average loss:2.622146\n",
      "epoch:827, Average loss:2.615488\n",
      "epoch:828, Average loss:2.682316\n",
      "epoch:829, Average loss:2.560165\n",
      "epoch:830, Average loss:2.657974\n",
      "epoch:831, Average loss:2.615595\n",
      "epoch:832, Average loss:2.648593\n",
      "epoch:833, Average loss:2.616367\n",
      "epoch:834, Average loss:2.665808\n",
      "epoch:835, Average loss:2.601368\n",
      "epoch:836, Average loss:2.592122\n",
      "epoch:837, Average loss:2.601363\n",
      "epoch:838, Average loss:2.670357\n",
      "epoch:839, Average loss:2.613199\n",
      "epoch:840, Average loss:2.65518\n",
      "epoch:841, Average loss:2.588972\n",
      "epoch:842, Average loss:2.617727\n",
      "epoch:843, Average loss:2.657875\n",
      "epoch:844, Average loss:2.589428\n",
      "epoch:845, Average loss:2.617923\n",
      "epoch:846, Average loss:2.62501\n",
      "epoch:847, Average loss:2.651488\n",
      "epoch:848, Average loss:2.604476\n",
      "epoch:849, Average loss:2.616095\n",
      "epoch:850, Average loss:2.551079\n",
      "epoch:851, Average loss:2.63559\n",
      "epoch:852, Average loss:2.639697\n",
      "epoch:853, Average loss:2.601455\n",
      "epoch:854, Average loss:2.578249\n",
      "epoch:855, Average loss:2.576207\n",
      "epoch:856, Average loss:2.588948\n",
      "epoch:857, Average loss:2.651787\n",
      "epoch:858, Average loss:2.615739\n",
      "epoch:859, Average loss:2.634333\n",
      "epoch:860, Average loss:2.602255\n",
      "epoch:861, Average loss:2.65946\n",
      "epoch:862, Average loss:2.593829\n",
      "epoch:863, Average loss:2.610674\n",
      "epoch:864, Average loss:2.606569\n",
      "epoch:865, Average loss:2.566203\n",
      "epoch:866, Average loss:2.616426\n",
      "epoch:867, Average loss:2.591946\n",
      "epoch:868, Average loss:2.563428\n",
      "epoch:869, Average loss:2.532833\n",
      "epoch:870, Average loss:2.56978\n",
      "epoch:871, Average loss:2.600083\n",
      "epoch:872, Average loss:2.538531\n",
      "epoch:873, Average loss:2.510916\n",
      "epoch:874, Average loss:2.567805\n",
      "epoch:875, Average loss:2.589353\n",
      "epoch:876, Average loss:2.579763\n",
      "epoch:877, Average loss:2.529788\n",
      "epoch:878, Average loss:2.595996\n",
      "epoch:879, Average loss:2.578038\n",
      "epoch:880, Average loss:2.577186\n",
      "epoch:881, Average loss:2.559617\n",
      "epoch:882, Average loss:2.556745\n",
      "epoch:883, Average loss:2.586395\n",
      "epoch:884, Average loss:2.571564\n",
      "epoch:885, Average loss:2.588309\n",
      "epoch:886, Average loss:2.599958\n",
      "epoch:887, Average loss:2.575846\n",
      "epoch:888, Average loss:2.521358\n",
      "epoch:889, Average loss:2.539303\n",
      "epoch:890, Average loss:2.574752\n",
      "epoch:891, Average loss:2.571508\n",
      "epoch:892, Average loss:2.562876\n",
      "epoch:893, Average loss:2.539112\n",
      "epoch:894, Average loss:2.590125\n",
      "epoch:895, Average loss:2.565749\n",
      "epoch:896, Average loss:2.583553\n",
      "epoch:897, Average loss:2.536703\n",
      "epoch:898, Average loss:2.54155\n",
      "epoch:899, Average loss:2.57325\n",
      "epoch:900, Average loss:2.513775\n",
      "epoch:901, Average loss:2.503056\n",
      "epoch:902, Average loss:2.617018\n",
      "epoch:903, Average loss:2.540891\n",
      "epoch:904, Average loss:2.595706\n",
      "epoch:905, Average loss:2.506801\n",
      "epoch:906, Average loss:2.623697\n",
      "epoch:907, Average loss:2.569132\n",
      "epoch:908, Average loss:2.516874\n",
      "epoch:909, Average loss:2.53204\n",
      "epoch:910, Average loss:2.559457\n",
      "epoch:911, Average loss:2.55803\n",
      "epoch:912, Average loss:2.589164\n",
      "epoch:913, Average loss:2.648874\n",
      "epoch:914, Average loss:2.523849\n",
      "epoch:915, Average loss:2.584176\n",
      "epoch:916, Average loss:2.510956\n",
      "epoch:917, Average loss:2.525778\n",
      "epoch:918, Average loss:2.561811\n",
      "epoch:919, Average loss:2.46853\n",
      "epoch:920, Average loss:2.538166\n",
      "epoch:921, Average loss:2.540741\n",
      "epoch:922, Average loss:2.580768\n",
      "epoch:923, Average loss:2.522642\n",
      "epoch:924, Average loss:2.551885\n",
      "epoch:925, Average loss:2.586469\n",
      "epoch:926, Average loss:2.550186\n",
      "epoch:927, Average loss:2.569616\n",
      "epoch:928, Average loss:2.501561\n",
      "epoch:929, Average loss:2.508795\n",
      "epoch:930, Average loss:2.621653\n",
      "epoch:931, Average loss:2.522685\n",
      "epoch:932, Average loss:2.522275\n",
      "epoch:933, Average loss:2.588847\n",
      "epoch:934, Average loss:2.520107\n",
      "epoch:935, Average loss:2.533189\n",
      "epoch:936, Average loss:2.556395\n",
      "epoch:937, Average loss:2.567735\n",
      "epoch:938, Average loss:2.564288\n",
      "epoch:939, Average loss:2.500735\n",
      "epoch:940, Average loss:2.565229\n",
      "epoch:941, Average loss:2.540119\n",
      "epoch:942, Average loss:2.504328\n",
      "epoch:943, Average loss:2.556633\n",
      "epoch:944, Average loss:2.461571\n",
      "epoch:945, Average loss:2.56207\n",
      "epoch:946, Average loss:2.472727\n",
      "epoch:947, Average loss:2.504734\n",
      "epoch:948, Average loss:2.555226\n",
      "epoch:949, Average loss:2.473974\n",
      "epoch:950, Average loss:2.545756\n",
      "epoch:951, Average loss:2.538812\n",
      "epoch:952, Average loss:2.620337\n",
      "epoch:953, Average loss:2.537388\n",
      "epoch:954, Average loss:2.534522\n",
      "epoch:955, Average loss:2.539578\n",
      "epoch:956, Average loss:2.561595\n",
      "epoch:957, Average loss:2.481948\n",
      "epoch:958, Average loss:2.502399\n",
      "epoch:959, Average loss:2.524743\n",
      "epoch:960, Average loss:2.539088\n",
      "epoch:961, Average loss:2.499627\n",
      "epoch:962, Average loss:2.482026\n",
      "epoch:963, Average loss:2.486677\n",
      "epoch:964, Average loss:2.510889\n",
      "epoch:965, Average loss:2.51598\n",
      "epoch:966, Average loss:2.520382\n",
      "epoch:967, Average loss:2.508379\n",
      "epoch:968, Average loss:2.568113\n",
      "epoch:969, Average loss:2.516289\n",
      "epoch:970, Average loss:2.512706\n",
      "epoch:971, Average loss:2.499671\n",
      "epoch:972, Average loss:2.504479\n",
      "epoch:973, Average loss:2.547001\n",
      "epoch:974, Average loss:2.500261\n",
      "epoch:975, Average loss:2.461815\n",
      "epoch:976, Average loss:2.516999\n",
      "epoch:977, Average loss:2.538228\n",
      "epoch:978, Average loss:2.543041\n",
      "epoch:979, Average loss:2.462165\n",
      "epoch:980, Average loss:2.505691\n",
      "epoch:981, Average loss:2.55319\n",
      "epoch:982, Average loss:2.504071\n",
      "epoch:983, Average loss:2.533133\n",
      "epoch:984, Average loss:2.530206\n",
      "epoch:985, Average loss:2.47714\n",
      "epoch:986, Average loss:2.469206\n",
      "epoch:987, Average loss:2.482933\n",
      "epoch:988, Average loss:2.544646\n",
      "epoch:989, Average loss:2.467253\n",
      "epoch:990, Average loss:2.443846\n",
      "epoch:991, Average loss:2.484556\n",
      "epoch:992, Average loss:2.5264\n",
      "epoch:993, Average loss:2.458152\n",
      "epoch:994, Average loss:2.531246\n",
      "epoch:995, Average loss:2.500192\n",
      "epoch:996, Average loss:2.479541\n",
      "epoch:997, Average loss:2.543984\n",
      "epoch:998, Average loss:2.485473\n",
      "epoch:999, Average loss:2.428802\n",
      "epoch:1000, Average loss:2.53067\n",
      "epoch:1001, Average loss:2.546847\n",
      "epoch:1002, Average loss:2.518615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1003, Average loss:2.464737\n",
      "epoch:1004, Average loss:2.485803\n",
      "epoch:1005, Average loss:2.487614\n",
      "epoch:1006, Average loss:2.566159\n",
      "epoch:1007, Average loss:2.500508\n",
      "epoch:1008, Average loss:2.462932\n",
      "epoch:1009, Average loss:2.520467\n",
      "epoch:1010, Average loss:2.462524\n",
      "epoch:1011, Average loss:2.459479\n",
      "epoch:1012, Average loss:2.506872\n",
      "epoch:1013, Average loss:2.553766\n",
      "epoch:1014, Average loss:2.498054\n",
      "epoch:1015, Average loss:2.476731\n",
      "epoch:1016, Average loss:2.455862\n",
      "epoch:1017, Average loss:2.496322\n",
      "epoch:1018, Average loss:2.487577\n",
      "epoch:1019, Average loss:2.477568\n",
      "epoch:1020, Average loss:2.456278\n",
      "epoch:1021, Average loss:2.471287\n",
      "epoch:1022, Average loss:2.464497\n",
      "epoch:1023, Average loss:2.482422\n",
      "epoch:1024, Average loss:2.482031\n",
      "epoch:1025, Average loss:2.455089\n",
      "epoch:1026, Average loss:2.467936\n",
      "epoch:1027, Average loss:2.493954\n",
      "epoch:1028, Average loss:2.48199\n",
      "epoch:1029, Average loss:2.435489\n",
      "epoch:1030, Average loss:2.461224\n",
      "epoch:1031, Average loss:2.437852\n",
      "epoch:1032, Average loss:2.451866\n",
      "epoch:1033, Average loss:2.540971\n",
      "epoch:1034, Average loss:2.463915\n",
      "epoch:1035, Average loss:2.496594\n",
      "epoch:1036, Average loss:2.467802\n",
      "epoch:1037, Average loss:2.450309\n",
      "epoch:1038, Average loss:2.449694\n",
      "epoch:1039, Average loss:2.504007\n",
      "epoch:1040, Average loss:2.504458\n",
      "epoch:1041, Average loss:2.480687\n",
      "epoch:1042, Average loss:2.488517\n",
      "epoch:1043, Average loss:2.505684\n",
      "epoch:1044, Average loss:2.519099\n",
      "epoch:1045, Average loss:2.516927\n",
      "epoch:1046, Average loss:2.425536\n",
      "epoch:1047, Average loss:2.456798\n",
      "epoch:1048, Average loss:2.431897\n",
      "epoch:1049, Average loss:2.451581\n",
      "epoch:1050, Average loss:2.50223\n",
      "epoch:1051, Average loss:2.408473\n",
      "epoch:1052, Average loss:2.442641\n",
      "epoch:1053, Average loss:2.458474\n",
      "epoch:1054, Average loss:2.493778\n",
      "epoch:1055, Average loss:2.467456\n",
      "epoch:1056, Average loss:2.426915\n",
      "epoch:1057, Average loss:2.490319\n",
      "epoch:1058, Average loss:2.490832\n",
      "epoch:1059, Average loss:2.431046\n",
      "epoch:1060, Average loss:2.436997\n",
      "epoch:1061, Average loss:2.455071\n",
      "epoch:1062, Average loss:2.415462\n",
      "epoch:1063, Average loss:2.45327\n",
      "epoch:1064, Average loss:2.539505\n",
      "epoch:1065, Average loss:2.491312\n",
      "epoch:1066, Average loss:2.435936\n",
      "epoch:1067, Average loss:2.478027\n",
      "epoch:1068, Average loss:2.517955\n",
      "epoch:1069, Average loss:2.43302\n",
      "epoch:1070, Average loss:2.427702\n",
      "epoch:1071, Average loss:2.459147\n",
      "epoch:1072, Average loss:2.444175\n",
      "epoch:1073, Average loss:2.484691\n",
      "epoch:1074, Average loss:2.499073\n",
      "epoch:1075, Average loss:2.44113\n",
      "epoch:1076, Average loss:2.454355\n",
      "epoch:1077, Average loss:2.416144\n",
      "epoch:1078, Average loss:2.435884\n",
      "epoch:1079, Average loss:2.409589\n",
      "epoch:1080, Average loss:2.479571\n",
      "epoch:1081, Average loss:2.463629\n",
      "epoch:1082, Average loss:2.383243\n",
      "epoch:1083, Average loss:2.425794\n",
      "epoch:1084, Average loss:2.482452\n",
      "epoch:1085, Average loss:2.465797\n",
      "epoch:1086, Average loss:2.504673\n",
      "epoch:1087, Average loss:2.476926\n",
      "epoch:1088, Average loss:2.385045\n",
      "epoch:1089, Average loss:2.515156\n",
      "epoch:1090, Average loss:2.42203\n",
      "epoch:1091, Average loss:2.444048\n",
      "epoch:1092, Average loss:2.485695\n",
      "epoch:1093, Average loss:2.406446\n",
      "epoch:1094, Average loss:2.443775\n",
      "epoch:1095, Average loss:2.424235\n",
      "epoch:1096, Average loss:2.41751\n",
      "epoch:1097, Average loss:2.430615\n",
      "epoch:1098, Average loss:2.438506\n",
      "epoch:1099, Average loss:2.428607\n",
      "epoch:1100, Average loss:2.419754\n",
      "epoch:1101, Average loss:2.409895\n",
      "epoch:1102, Average loss:2.469389\n",
      "epoch:1103, Average loss:2.393254\n",
      "epoch:1104, Average loss:2.44954\n",
      "epoch:1105, Average loss:2.401056\n",
      "epoch:1106, Average loss:2.400297\n",
      "epoch:1107, Average loss:2.469979\n",
      "epoch:1108, Average loss:2.414373\n",
      "epoch:1109, Average loss:2.441706\n",
      "epoch:1110, Average loss:2.454334\n",
      "epoch:1111, Average loss:2.420297\n",
      "epoch:1112, Average loss:2.457647\n",
      "epoch:1113, Average loss:2.40623\n",
      "epoch:1114, Average loss:2.39467\n",
      "epoch:1115, Average loss:2.432235\n",
      "epoch:1116, Average loss:2.423323\n",
      "epoch:1117, Average loss:2.435196\n",
      "epoch:1118, Average loss:2.461811\n",
      "epoch:1119, Average loss:2.414574\n",
      "epoch:1120, Average loss:2.46428\n",
      "epoch:1121, Average loss:2.361629\n",
      "epoch:1122, Average loss:2.445235\n",
      "epoch:1123, Average loss:2.445654\n",
      "epoch:1124, Average loss:2.387652\n",
      "epoch:1125, Average loss:2.413021\n",
      "epoch:1126, Average loss:2.411791\n",
      "epoch:1127, Average loss:2.41349\n",
      "epoch:1128, Average loss:2.403861\n",
      "epoch:1129, Average loss:2.417886\n",
      "epoch:1130, Average loss:2.42494\n",
      "epoch:1131, Average loss:2.371667\n",
      "epoch:1132, Average loss:2.460536\n",
      "epoch:1133, Average loss:2.445836\n",
      "epoch:1134, Average loss:2.478974\n",
      "epoch:1135, Average loss:2.399306\n",
      "epoch:1136, Average loss:2.47336\n",
      "epoch:1137, Average loss:2.421507\n",
      "epoch:1138, Average loss:2.436207\n",
      "epoch:1139, Average loss:2.459062\n",
      "epoch:1140, Average loss:2.436848\n",
      "epoch:1141, Average loss:2.392411\n",
      "epoch:1142, Average loss:2.471674\n",
      "epoch:1143, Average loss:2.379868\n",
      "epoch:1144, Average loss:2.355715\n",
      "epoch:1145, Average loss:2.410071\n",
      "epoch:1146, Average loss:2.47253\n",
      "epoch:1147, Average loss:2.423135\n",
      "epoch:1148, Average loss:2.41306\n",
      "epoch:1149, Average loss:2.41929\n",
      "epoch:1150, Average loss:2.385915\n",
      "epoch:1151, Average loss:2.398435\n",
      "epoch:1152, Average loss:2.399207\n",
      "epoch:1153, Average loss:2.36995\n",
      "epoch:1154, Average loss:2.370008\n",
      "epoch:1155, Average loss:2.405155\n",
      "epoch:1156, Average loss:2.378943\n",
      "epoch:1157, Average loss:2.388588\n",
      "epoch:1158, Average loss:2.363055\n",
      "epoch:1159, Average loss:2.378271\n",
      "epoch:1160, Average loss:2.344911\n",
      "epoch:1161, Average loss:2.399037\n",
      "epoch:1162, Average loss:2.352049\n",
      "epoch:1163, Average loss:2.387057\n",
      "epoch:1164, Average loss:2.37894\n",
      "epoch:1165, Average loss:2.401154\n",
      "epoch:1166, Average loss:2.389032\n",
      "epoch:1167, Average loss:2.470544\n",
      "epoch:1168, Average loss:2.46384\n",
      "epoch:1169, Average loss:2.437854\n",
      "epoch:1170, Average loss:2.468092\n",
      "epoch:1171, Average loss:2.359841\n",
      "epoch:1172, Average loss:2.342323\n",
      "epoch:1173, Average loss:2.397988\n",
      "epoch:1174, Average loss:2.363996\n",
      "epoch:1175, Average loss:2.335162\n",
      "epoch:1176, Average loss:2.404166\n",
      "epoch:1177, Average loss:2.375059\n",
      "epoch:1178, Average loss:2.409006\n",
      "epoch:1179, Average loss:2.390422\n",
      "epoch:1180, Average loss:2.29671\n",
      "epoch:1181, Average loss:2.402918\n",
      "epoch:1182, Average loss:2.42698\n",
      "epoch:1183, Average loss:2.369156\n",
      "epoch:1184, Average loss:2.377756\n",
      "epoch:1185, Average loss:2.420319\n",
      "epoch:1186, Average loss:2.413342\n",
      "epoch:1187, Average loss:2.389057\n",
      "epoch:1188, Average loss:2.406411\n",
      "epoch:1189, Average loss:2.333626\n",
      "epoch:1190, Average loss:2.346081\n",
      "epoch:1191, Average loss:2.3928\n",
      "epoch:1192, Average loss:2.386221\n",
      "epoch:1193, Average loss:2.39968\n",
      "epoch:1194, Average loss:2.376633\n",
      "epoch:1195, Average loss:2.373306\n",
      "epoch:1196, Average loss:2.352091\n",
      "epoch:1197, Average loss:2.349503\n",
      "epoch:1198, Average loss:2.380596\n",
      "epoch:1199, Average loss:2.359505\n",
      "epoch:1200, Average loss:2.355267\n",
      "epoch:1201, Average loss:2.387977\n",
      "epoch:1202, Average loss:2.367491\n",
      "epoch:1203, Average loss:2.389844\n",
      "epoch:1204, Average loss:2.353214\n",
      "epoch:1205, Average loss:2.372025\n",
      "epoch:1206, Average loss:2.389388\n",
      "epoch:1207, Average loss:2.33717\n",
      "epoch:1208, Average loss:2.335291\n",
      "epoch:1209, Average loss:2.338892\n",
      "epoch:1210, Average loss:2.381304\n",
      "epoch:1211, Average loss:2.403327\n",
      "epoch:1212, Average loss:2.440548\n",
      "epoch:1213, Average loss:2.345313\n",
      "epoch:1214, Average loss:2.340645\n",
      "epoch:1215, Average loss:2.349461\n",
      "epoch:1216, Average loss:2.34174\n",
      "epoch:1217, Average loss:2.32406\n",
      "epoch:1218, Average loss:2.397128\n",
      "epoch:1219, Average loss:2.347863\n",
      "epoch:1220, Average loss:2.376933\n",
      "epoch:1221, Average loss:2.405284\n",
      "epoch:1222, Average loss:2.395129\n",
      "epoch:1223, Average loss:2.328063\n",
      "epoch:1224, Average loss:2.340176\n",
      "epoch:1225, Average loss:2.380852\n",
      "epoch:1226, Average loss:2.361408\n",
      "epoch:1227, Average loss:2.280796\n",
      "epoch:1228, Average loss:2.311376\n",
      "epoch:1229, Average loss:2.393647\n",
      "epoch:1230, Average loss:2.297658\n",
      "epoch:1231, Average loss:2.355703\n",
      "epoch:1232, Average loss:2.322886\n",
      "epoch:1233, Average loss:2.341925\n",
      "epoch:1234, Average loss:2.360409\n",
      "epoch:1235, Average loss:2.302142\n",
      "epoch:1236, Average loss:2.394791\n",
      "epoch:1237, Average loss:2.337649\n",
      "epoch:1238, Average loss:2.325708\n",
      "epoch:1239, Average loss:2.362417\n",
      "epoch:1240, Average loss:2.43706\n",
      "epoch:1241, Average loss:2.305619\n",
      "epoch:1242, Average loss:2.330318\n",
      "epoch:1243, Average loss:2.36482\n",
      "epoch:1244, Average loss:2.384577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1245, Average loss:2.34594\n",
      "epoch:1246, Average loss:2.279485\n",
      "epoch:1247, Average loss:2.346515\n",
      "epoch:1248, Average loss:2.337735\n",
      "epoch:1249, Average loss:2.316196\n",
      "epoch:1250, Average loss:2.357784\n",
      "epoch:1251, Average loss:2.368098\n",
      "epoch:1252, Average loss:2.349739\n",
      "epoch:1253, Average loss:2.398829\n",
      "epoch:1254, Average loss:2.315246\n",
      "epoch:1255, Average loss:2.38472\n",
      "epoch:1256, Average loss:2.403771\n",
      "epoch:1257, Average loss:2.345105\n",
      "epoch:1258, Average loss:2.268143\n",
      "epoch:1259, Average loss:2.361316\n",
      "epoch:1260, Average loss:2.286118\n",
      "epoch:1261, Average loss:2.292104\n",
      "epoch:1262, Average loss:2.380831\n",
      "epoch:1263, Average loss:2.325817\n",
      "epoch:1264, Average loss:2.278586\n",
      "epoch:1265, Average loss:2.369328\n",
      "epoch:1266, Average loss:2.343569\n",
      "epoch:1267, Average loss:2.336793\n",
      "epoch:1268, Average loss:2.330253\n",
      "epoch:1269, Average loss:2.325107\n",
      "epoch:1270, Average loss:2.373854\n",
      "epoch:1271, Average loss:2.320316\n",
      "epoch:1272, Average loss:2.338451\n",
      "epoch:1273, Average loss:2.352674\n",
      "epoch:1274, Average loss:2.358466\n",
      "epoch:1275, Average loss:2.30951\n",
      "epoch:1276, Average loss:2.377042\n",
      "epoch:1277, Average loss:2.306696\n",
      "epoch:1278, Average loss:2.352357\n",
      "epoch:1279, Average loss:2.29919\n",
      "epoch:1280, Average loss:2.292466\n",
      "epoch:1281, Average loss:2.288918\n",
      "epoch:1282, Average loss:2.320086\n",
      "epoch:1283, Average loss:2.333339\n",
      "epoch:1284, Average loss:2.340768\n",
      "epoch:1285, Average loss:2.286789\n",
      "epoch:1286, Average loss:2.348683\n",
      "epoch:1287, Average loss:2.300276\n",
      "epoch:1288, Average loss:2.349467\n",
      "epoch:1289, Average loss:2.327273\n",
      "epoch:1290, Average loss:2.294787\n",
      "epoch:1291, Average loss:2.334153\n",
      "epoch:1292, Average loss:2.303466\n",
      "epoch:1293, Average loss:2.282309\n",
      "epoch:1294, Average loss:2.331293\n",
      "epoch:1295, Average loss:2.304771\n",
      "epoch:1296, Average loss:2.297667\n",
      "epoch:1297, Average loss:2.308641\n",
      "epoch:1298, Average loss:2.312466\n",
      "epoch:1299, Average loss:2.373592\n",
      "epoch:1300, Average loss:2.316685\n",
      "epoch:1301, Average loss:2.322984\n",
      "epoch:1302, Average loss:2.329134\n",
      "epoch:1303, Average loss:2.286795\n",
      "epoch:1304, Average loss:2.327428\n",
      "epoch:1305, Average loss:2.315898\n",
      "epoch:1306, Average loss:2.306541\n",
      "epoch:1307, Average loss:2.269391\n",
      "epoch:1308, Average loss:2.302394\n",
      "epoch:1309, Average loss:2.290847\n",
      "epoch:1310, Average loss:2.274449\n",
      "epoch:1311, Average loss:2.243669\n",
      "epoch:1312, Average loss:2.332866\n",
      "epoch:1313, Average loss:2.265707\n",
      "epoch:1314, Average loss:2.329237\n",
      "epoch:1315, Average loss:2.263321\n",
      "epoch:1316, Average loss:2.306939\n",
      "epoch:1317, Average loss:2.338044\n",
      "epoch:1318, Average loss:2.253984\n",
      "epoch:1319, Average loss:2.236181\n",
      "epoch:1320, Average loss:2.317178\n",
      "epoch:1321, Average loss:2.261686\n",
      "epoch:1322, Average loss:2.316255\n",
      "epoch:1323, Average loss:2.314257\n",
      "epoch:1324, Average loss:2.294989\n",
      "epoch:1325, Average loss:2.287863\n",
      "epoch:1326, Average loss:2.27363\n",
      "epoch:1327, Average loss:2.292213\n",
      "epoch:1328, Average loss:2.295401\n",
      "epoch:1329, Average loss:2.282042\n",
      "epoch:1330, Average loss:2.283104\n",
      "epoch:1331, Average loss:2.29484\n",
      "epoch:1332, Average loss:2.288137\n",
      "epoch:1333, Average loss:2.28572\n",
      "epoch:1334, Average loss:2.306708\n",
      "epoch:1335, Average loss:2.287818\n",
      "epoch:1336, Average loss:2.243445\n",
      "epoch:1337, Average loss:2.325532\n",
      "epoch:1338, Average loss:2.251971\n",
      "epoch:1339, Average loss:2.311639\n",
      "epoch:1340, Average loss:2.285067\n",
      "epoch:1341, Average loss:2.306649\n",
      "epoch:1342, Average loss:2.319108\n",
      "epoch:1343, Average loss:2.249838\n",
      "epoch:1344, Average loss:2.275953\n",
      "epoch:1345, Average loss:2.26623\n",
      "epoch:1346, Average loss:2.314523\n",
      "epoch:1347, Average loss:2.233345\n",
      "epoch:1348, Average loss:2.304481\n",
      "epoch:1349, Average loss:2.285858\n",
      "epoch:1350, Average loss:2.268585\n",
      "epoch:1351, Average loss:2.296153\n",
      "epoch:1352, Average loss:2.242546\n",
      "epoch:1353, Average loss:2.298\n",
      "epoch:1354, Average loss:2.230819\n",
      "epoch:1355, Average loss:2.287148\n",
      "epoch:1356, Average loss:2.305864\n",
      "epoch:1357, Average loss:2.346653\n",
      "epoch:1358, Average loss:2.257548\n",
      "epoch:1359, Average loss:2.265504\n",
      "epoch:1360, Average loss:2.296119\n",
      "epoch:1361, Average loss:2.294328\n",
      "epoch:1362, Average loss:2.279479\n",
      "epoch:1363, Average loss:2.282642\n",
      "epoch:1364, Average loss:2.263815\n",
      "epoch:1365, Average loss:2.273997\n",
      "epoch:1366, Average loss:2.234174\n",
      "epoch:1367, Average loss:2.243161\n",
      "epoch:1368, Average loss:2.235275\n",
      "epoch:1369, Average loss:2.275012\n",
      "epoch:1370, Average loss:2.240673\n",
      "epoch:1371, Average loss:2.29631\n",
      "epoch:1372, Average loss:2.251821\n",
      "epoch:1373, Average loss:2.198748\n",
      "epoch:1374, Average loss:2.234573\n",
      "epoch:1375, Average loss:2.263162\n",
      "epoch:1376, Average loss:2.241447\n",
      "epoch:1377, Average loss:2.271779\n",
      "epoch:1378, Average loss:2.245078\n",
      "epoch:1379, Average loss:2.288553\n",
      "epoch:1380, Average loss:2.250685\n",
      "epoch:1381, Average loss:2.259776\n",
      "epoch:1382, Average loss:2.278598\n",
      "epoch:1383, Average loss:2.261531\n",
      "epoch:1384, Average loss:2.257539\n",
      "epoch:1385, Average loss:2.268844\n",
      "epoch:1386, Average loss:2.250386\n",
      "epoch:1387, Average loss:2.256326\n",
      "epoch:1388, Average loss:2.258443\n",
      "epoch:1389, Average loss:2.328397\n",
      "epoch:1390, Average loss:2.225666\n",
      "epoch:1391, Average loss:2.261991\n",
      "epoch:1392, Average loss:2.269002\n",
      "epoch:1393, Average loss:2.230875\n",
      "epoch:1394, Average loss:2.269789\n",
      "epoch:1395, Average loss:2.257637\n",
      "epoch:1396, Average loss:2.274081\n",
      "epoch:1397, Average loss:2.239473\n",
      "epoch:1398, Average loss:2.20743\n",
      "epoch:1399, Average loss:2.24165\n",
      "epoch:1400, Average loss:2.247002\n",
      "epoch:1401, Average loss:2.25942\n",
      "epoch:1402, Average loss:2.307896\n",
      "epoch:1403, Average loss:2.242357\n",
      "epoch:1404, Average loss:2.268149\n",
      "epoch:1405, Average loss:2.279717\n",
      "epoch:1406, Average loss:2.213241\n",
      "epoch:1407, Average loss:2.237553\n",
      "epoch:1408, Average loss:2.200339\n",
      "epoch:1409, Average loss:2.223533\n",
      "epoch:1410, Average loss:2.259679\n",
      "epoch:1411, Average loss:2.233317\n",
      "epoch:1412, Average loss:2.203391\n",
      "epoch:1413, Average loss:2.211303\n",
      "epoch:1414, Average loss:2.258776\n",
      "epoch:1415, Average loss:2.202347\n",
      "epoch:1416, Average loss:2.28875\n",
      "epoch:1417, Average loss:2.17716\n",
      "epoch:1418, Average loss:2.260606\n",
      "epoch:1419, Average loss:2.28473\n",
      "epoch:1420, Average loss:2.24665\n",
      "epoch:1421, Average loss:2.18096\n",
      "epoch:1422, Average loss:2.263513\n",
      "epoch:1423, Average loss:2.217846\n",
      "epoch:1424, Average loss:2.238223\n",
      "epoch:1425, Average loss:2.237859\n",
      "epoch:1426, Average loss:2.285351\n",
      "epoch:1427, Average loss:2.253241\n",
      "epoch:1428, Average loss:2.267772\n",
      "epoch:1429, Average loss:2.254772\n",
      "epoch:1430, Average loss:2.201391\n",
      "epoch:1431, Average loss:2.236491\n",
      "epoch:1432, Average loss:2.209967\n",
      "epoch:1433, Average loss:2.231556\n",
      "epoch:1434, Average loss:2.272481\n",
      "epoch:1435, Average loss:2.238703\n",
      "epoch:1436, Average loss:2.263848\n",
      "epoch:1437, Average loss:2.23924\n",
      "epoch:1438, Average loss:2.278513\n",
      "epoch:1439, Average loss:2.245403\n",
      "epoch:1440, Average loss:2.192311\n",
      "epoch:1441, Average loss:2.244222\n",
      "epoch:1442, Average loss:2.25892\n",
      "epoch:1443, Average loss:2.237338\n",
      "epoch:1444, Average loss:2.208954\n"
     ]
    }
   ],
   "source": [
    "avg_loss_record = []\n",
    "list_batch_labels = []\n",
    "list_batch_inputs = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #Shuffle the list of nodes at the start of each epoch\n",
    "        random.shuffle(list_of_nodes)\n",
    "        random_walks = []\n",
    "        \n",
    "        for node in list_of_nodes:\n",
    "            #Step through each node and conduct a random walk about it of length max_step\n",
    "            path = randomWalk(web_graph, node, 0, max_step, [node])\n",
    "            \n",
    "            path = [domain_inv_map[i] for i in path]\n",
    "            random_walks.append(path)\n",
    "        \n",
    "        data_windows = np.array(random_walks)\n",
    "                \n",
    "        target = data_windows[:,window_size]\n",
    "\n",
    "        left_window = data_windows[:,:window_size]\n",
    "\n",
    "        right_window = data_windows[:,window_size+1:]\n",
    "\n",
    "        context_window = np.concatenate([left_window, right_window], axis=1)\n",
    "            \n",
    "        for step in range(num_steps):\n",
    "\n",
    "            batch_data = generateBatch(batch_size, num_skips, context_window, target, step)\n",
    "            batch_inputs = [row[0] for row in batch_data]\n",
    "            batch_labels = [row[1] for row in batch_data]\n",
    "           \n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "            list_batch_labels.append([batch_labels])\n",
    "            list_batch_inputs.append([batch_inputs])\n",
    "            \n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            average_loss += loss_val\n",
    "         \n",
    "        if epoch%1==0: \n",
    "            \n",
    "            avg_loss_record.append(float(average_loss)/num_steps)\n",
    "            print('epoch:%d, Average loss:%.7g' % (epoch, float(average_loss)/num_steps))\n",
    "        \n",
    "        average_loss = 0\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAF3CAYAAAB5QUrKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x220fb7c4828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_loss = np.array(avg_loss_record)\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot([i for \n",
    "          i in range(len(list_loss))], list_loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "#Create cosine similarity matrix\n",
    "\n",
    "mult_vector = tf.matmul(vector, vector, transpose_b=True)\n",
    "sim_matrix = tf.acos(mult_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    matrices = sess.run([sim_matrix, mult_vector] , feed_dict={vector:final_embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = matrices[0]\n",
    "\n",
    "np.fill_diagonal(sim_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_embeddings, open('embeddings_test_domain_graph_undirected.pkl', 'wb'))\n",
    "pickle.dump(sim_matrix, open('cosine_matrix_test_domain_graph_undirected.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
