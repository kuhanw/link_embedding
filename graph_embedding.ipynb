{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "\n",
    "from web_crawler import grabDomainRoot\n",
    "\n",
    "'''\n",
    "randomWalk(graph, initial_node, step, max_step, path)\n",
    "\n",
    "Function to take a random walk from a given node\n",
    "\n",
    "graph: networkx graph, graph from which to random through\n",
    "initial node: string, initial node to begin the walk\n",
    "step: int, current step of walk\n",
    "max_step: int, maximum number of steps to take in walk\n",
    "path:, list, current path taken in the walk\n",
    "'''\n",
    "def randomWalk(graph, initial_node, step, max_step, path):\n",
    " \n",
    "    if step>= max_step: \n",
    "        return path\n",
    "    \n",
    "    adjacent_nodes = [i for i in graph.neighbors(initial_node)]\n",
    "    \n",
    "    next_node = random.sample(adjacent_nodes, 1)[0]\n",
    "    \n",
    "    path.append(next_node)\n",
    "    \n",
    "    return randomWalk(graph, next_node, step+1, max_step, path)\n",
    "\n",
    "'''\n",
    "generateBatch(batch_size, num_context_per_label, context_window, target, step)\n",
    "\n",
    "batch_size: int, batch size for training\n",
    "num_context_per_label: int, how many context examples to use per label (the label is the target) \n",
    "can't be greater than the context window size\n",
    "context_window: int, size of the context window \n",
    "target: array, the list of targets for each context window\n",
    "step: int, counter for how many times to step through the same context and target data\n",
    "\n",
    "Generate the batch data for training. For each \"context window\", randomly sample a\n",
    "set of context elements and configure them as training data by constructing column data of,\n",
    "\n",
    "[target_0, context_0]\n",
    "[target_0, context_1]\n",
    "[target_0, context_3]\n",
    "...\n",
    "[target_n, context_3]\n",
    "[target_n, context_2]\n",
    "[target_n, context_1]\n",
    "\n",
    "'''\n",
    "def generateBatch(batch_size, num_context_per_label, context_window, target, step):\n",
    "\n",
    "    batch = []\n",
    "    passes_through_batch = batch_size//num_context_per_label\n",
    "    \n",
    "    for window_idx in range(passes_through_batch):\n",
    "\n",
    "        current_window = list(context_window[window_idx + passes_through_batch*step])\n",
    "        current_target = target[window_idx + passes_through_batch*step]\n",
    "        context_samples = random.sample(current_window, num_context_per_label)\n",
    "        data_samples =  [[context_sample, [current_target]] for context_sample in context_samples]\n",
    "\n",
    "        for data_sample in data_samples:\n",
    "            batch.append(data_sample)\n",
    "            \n",
    "    return batch\n",
    "\n",
    "black_list = ['@', ':/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = pickle.load(open('crawler_results/graph_calls_620000_stack_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n",
      "NO BASE URL\n"
     ]
    }
   ],
   "source": [
    "#Create a graph out of the connections\n",
    "\n",
    "#web_graph = nx.DiGraph()\n",
    "web_graph = nx.Graph()\n",
    "for node in graph_file.keys():\n",
    "    for idx in range(0, len(graph_file[node]), 3):\n",
    "        key = graph_file[node][idx]\n",
    "\n",
    "        domain_node = grabDomainRoot(node)\n",
    "        domain_key = grabDomainRoot(key)\n",
    "        \n",
    "        if domain_node is None or domain_key is None: \n",
    "            continue\n",
    "        if True in [i in domain_node for i in black_list] or True in [i in domain_key for i in black_list]:\n",
    "            continue\n",
    "\n",
    "        web_graph.add_edge(domain_node, domain_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nodes = [i for i in web_graph.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_inv_map = {idx:i for idx, i in enumerate(list_of_nodes)}\n",
    "domain_map = {i:idx for idx, i in enumerate(list_of_nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8943 nodes, 8943 key_domain dict\n"
     ]
    }
   ],
   "source": [
    "#Sanity check\n",
    "#print('%d nodes, %d dict terms, %d key_domain dict' % (len(list_of_nodes), len(vocab_dict), len(key_domain_dict)))\n",
    "print('%d nodes, %d key_domain dict' % (len(list_of_nodes), len(domain_inv_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = 4# Window size and max_step must be connected\n",
    "\n",
    "num_skips = 2 #The number of context examples per label to create x-y data out of \n",
    "#i.e. the number of rows of \"data\" per window, label combo\n",
    "window_size = max_step//2 #where max step must be even\n",
    "embedding_size = 32  #Dimension of the embedding vector.\n",
    "vocabulary_size = len(web_graph.nodes())\n",
    "\n",
    "num_sampled = 64 #Number of negative examples to sample. \n",
    "#As this number goes to the total number of samples it reproduces softmax, \n",
    "#this not quite correct as we still doing binary classification, except now we give every negative example to test against,\n",
    "#as opposed to true multi-class classification\n",
    "batch_size = 64 #must be a multiple of num_skips\n",
    "num_steps = int(len(list_of_nodes)/batch_size)\n",
    "n_epochs = 10000 #This controls the number of walks from each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8943 nodes, 139 steps per epoch\n"
     ]
    }
   ],
   "source": [
    "print ('%d nodes, %d steps per epoch' % (len(list_of_nodes), num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "epoch:0, Average loss:201.0651\n",
      "epoch:1, Average loss:180.1612\n",
      "epoch:2, Average loss:167.784\n",
      "epoch:3, Average loss:161.2119\n",
      "epoch:4, Average loss:152.6324\n",
      "epoch:5, Average loss:142.3703\n",
      "epoch:6, Average loss:138.4955\n",
      "epoch:7, Average loss:129.1191\n",
      "epoch:8, Average loss:124.9444\n",
      "epoch:9, Average loss:119.9018\n",
      "epoch:10, Average loss:115.8541\n",
      "epoch:11, Average loss:108.5474\n",
      "epoch:12, Average loss:106.3352\n",
      "epoch:13, Average loss:99.02678\n",
      "epoch:14, Average loss:94.69514\n",
      "epoch:15, Average loss:91.58173\n",
      "epoch:16, Average loss:88.32408\n",
      "epoch:17, Average loss:86.52755\n",
      "epoch:18, Average loss:81.67401\n",
      "epoch:19, Average loss:77.82794\n",
      "epoch:20, Average loss:75.2215\n",
      "epoch:21, Average loss:71.08004\n",
      "epoch:22, Average loss:68.647\n",
      "epoch:23, Average loss:66.9712\n",
      "epoch:24, Average loss:64.81722\n",
      "epoch:25, Average loss:62.66742\n",
      "epoch:26, Average loss:59.60324\n",
      "epoch:27, Average loss:58.72591\n",
      "epoch:28, Average loss:54.34379\n",
      "epoch:29, Average loss:54.64056\n",
      "epoch:30, Average loss:50.70596\n",
      "epoch:31, Average loss:50.71321\n",
      "epoch:32, Average loss:49.5081\n",
      "epoch:33, Average loss:47.92974\n",
      "epoch:34, Average loss:46.43287\n",
      "epoch:35, Average loss:43.09906\n",
      "epoch:36, Average loss:43.76121\n",
      "epoch:37, Average loss:41.88367\n",
      "epoch:38, Average loss:40.77678\n",
      "epoch:39, Average loss:41.17244\n",
      "epoch:40, Average loss:39.23832\n",
      "epoch:41, Average loss:37.85771\n",
      "epoch:42, Average loss:37.24628\n",
      "epoch:43, Average loss:36.10667\n",
      "epoch:44, Average loss:36.24379\n",
      "epoch:45, Average loss:33.43665\n",
      "epoch:46, Average loss:33.7134\n",
      "epoch:47, Average loss:32.10096\n",
      "epoch:48, Average loss:30.68781\n",
      "epoch:49, Average loss:31.53707\n",
      "epoch:50, Average loss:30.47503\n",
      "epoch:51, Average loss:29.43881\n",
      "epoch:52, Average loss:29.04997\n",
      "epoch:53, Average loss:27.81308\n",
      "epoch:54, Average loss:28.11223\n",
      "epoch:55, Average loss:26.46206\n",
      "epoch:56, Average loss:25.39162\n",
      "epoch:57, Average loss:25.37864\n",
      "epoch:58, Average loss:25.05699\n",
      "epoch:59, Average loss:24.71055\n",
      "epoch:60, Average loss:23.28006\n",
      "epoch:61, Average loss:22.25153\n",
      "epoch:62, Average loss:22.46254\n",
      "epoch:63, Average loss:22.31252\n",
      "epoch:64, Average loss:21.46831\n",
      "epoch:65, Average loss:21.05315\n",
      "epoch:66, Average loss:20.9117\n",
      "epoch:67, Average loss:19.76123\n",
      "epoch:68, Average loss:19.24379\n",
      "epoch:69, Average loss:19.33585\n",
      "epoch:70, Average loss:18.36549\n",
      "epoch:71, Average loss:18.3841\n",
      "epoch:72, Average loss:17.50914\n",
      "epoch:73, Average loss:17.07367\n",
      "epoch:74, Average loss:17.04745\n",
      "epoch:75, Average loss:16.96153\n",
      "epoch:76, Average loss:16.4365\n",
      "epoch:77, Average loss:16.00695\n",
      "epoch:78, Average loss:15.7971\n",
      "epoch:79, Average loss:15.575\n",
      "epoch:80, Average loss:15.10152\n",
      "epoch:81, Average loss:15.01693\n",
      "epoch:82, Average loss:14.94391\n",
      "epoch:83, Average loss:14.38157\n",
      "epoch:84, Average loss:14.00567\n",
      "epoch:85, Average loss:13.84732\n",
      "epoch:86, Average loss:13.0623\n",
      "epoch:87, Average loss:13.00055\n",
      "epoch:88, Average loss:12.90749\n",
      "epoch:89, Average loss:12.72554\n",
      "epoch:90, Average loss:11.98394\n",
      "epoch:91, Average loss:12.31154\n",
      "epoch:92, Average loss:12.10986\n",
      "epoch:93, Average loss:11.83601\n",
      "epoch:94, Average loss:11.39934\n",
      "epoch:95, Average loss:10.88266\n",
      "epoch:96, Average loss:11.33726\n",
      "epoch:97, Average loss:10.95049\n",
      "epoch:98, Average loss:10.3342\n",
      "epoch:99, Average loss:10.20088\n",
      "epoch:100, Average loss:10.38124\n",
      "epoch:101, Average loss:10.11704\n",
      "epoch:102, Average loss:9.73208\n",
      "epoch:103, Average loss:9.407442\n",
      "epoch:104, Average loss:9.437505\n",
      "epoch:105, Average loss:9.356387\n",
      "epoch:106, Average loss:9.105576\n",
      "epoch:107, Average loss:8.832881\n",
      "epoch:108, Average loss:8.918378\n",
      "epoch:109, Average loss:8.722185\n",
      "epoch:110, Average loss:8.609481\n",
      "epoch:111, Average loss:8.542295\n",
      "epoch:112, Average loss:8.155122\n",
      "epoch:113, Average loss:8.118804\n",
      "epoch:114, Average loss:8.116036\n",
      "epoch:115, Average loss:7.717801\n",
      "epoch:116, Average loss:7.651708\n",
      "epoch:117, Average loss:7.716107\n",
      "epoch:118, Average loss:7.762791\n",
      "epoch:119, Average loss:7.305063\n",
      "epoch:120, Average loss:7.559858\n",
      "epoch:121, Average loss:7.331591\n",
      "epoch:122, Average loss:7.263459\n",
      "epoch:123, Average loss:7.03832\n",
      "epoch:124, Average loss:6.989606\n",
      "epoch:125, Average loss:6.691471\n",
      "epoch:126, Average loss:6.864502\n",
      "epoch:127, Average loss:6.816136\n",
      "epoch:128, Average loss:6.452119\n",
      "epoch:129, Average loss:6.560413\n",
      "epoch:130, Average loss:6.497292\n",
      "epoch:131, Average loss:6.347576\n",
      "epoch:132, Average loss:6.241972\n",
      "epoch:133, Average loss:6.252567\n",
      "epoch:134, Average loss:6.135785\n",
      "epoch:135, Average loss:6.07788\n",
      "epoch:136, Average loss:6.053977\n",
      "epoch:137, Average loss:5.900355\n",
      "epoch:138, Average loss:6.071346\n",
      "epoch:139, Average loss:5.769924\n",
      "epoch:140, Average loss:5.644351\n",
      "epoch:141, Average loss:5.722909\n",
      "epoch:142, Average loss:5.499218\n",
      "epoch:143, Average loss:5.756057\n",
      "epoch:144, Average loss:5.611436\n",
      "epoch:145, Average loss:5.500435\n",
      "epoch:146, Average loss:5.403626\n",
      "epoch:147, Average loss:5.440416\n",
      "epoch:148, Average loss:5.478868\n",
      "epoch:149, Average loss:5.288534\n",
      "epoch:150, Average loss:5.172744\n",
      "epoch:151, Average loss:5.223229\n",
      "epoch:152, Average loss:5.164067\n",
      "epoch:153, Average loss:5.201249\n",
      "epoch:154, Average loss:5.115931\n",
      "epoch:155, Average loss:5.141007\n",
      "epoch:156, Average loss:4.922977\n",
      "epoch:157, Average loss:5.019083\n",
      "epoch:158, Average loss:4.988473\n",
      "epoch:159, Average loss:4.936355\n",
      "epoch:160, Average loss:4.950652\n",
      "epoch:161, Average loss:4.726018\n",
      "epoch:162, Average loss:4.77053\n",
      "epoch:163, Average loss:4.807525\n",
      "epoch:164, Average loss:4.753605\n",
      "epoch:165, Average loss:4.659281\n",
      "epoch:166, Average loss:4.634065\n",
      "epoch:167, Average loss:4.661822\n",
      "epoch:168, Average loss:4.569324\n",
      "epoch:169, Average loss:4.53392\n",
      "epoch:170, Average loss:4.539425\n",
      "epoch:171, Average loss:4.555039\n",
      "epoch:172, Average loss:4.515184\n",
      "epoch:173, Average loss:4.384088\n",
      "epoch:174, Average loss:4.490901\n",
      "epoch:175, Average loss:4.446208\n",
      "epoch:176, Average loss:4.409692\n",
      "epoch:177, Average loss:4.39345\n",
      "epoch:178, Average loss:4.348464\n",
      "epoch:179, Average loss:4.335888\n",
      "epoch:180, Average loss:4.343469\n",
      "epoch:181, Average loss:4.298602\n",
      "epoch:182, Average loss:4.350706\n",
      "epoch:183, Average loss:4.308999\n",
      "epoch:184, Average loss:4.228463\n",
      "epoch:185, Average loss:4.309522\n",
      "epoch:186, Average loss:4.267646\n",
      "epoch:187, Average loss:4.225572\n",
      "epoch:188, Average loss:4.240403\n",
      "epoch:189, Average loss:4.051926\n",
      "epoch:190, Average loss:4.115166\n",
      "epoch:191, Average loss:4.121716\n",
      "epoch:192, Average loss:4.050109\n",
      "epoch:193, Average loss:4.153056\n",
      "epoch:194, Average loss:4.139537\n",
      "epoch:195, Average loss:4.082554\n",
      "epoch:196, Average loss:4.024042\n",
      "epoch:197, Average loss:4.03789\n",
      "epoch:198, Average loss:4.084983\n",
      "epoch:199, Average loss:4.022283\n",
      "epoch:200, Average loss:3.915237\n",
      "epoch:201, Average loss:4.006614\n",
      "epoch:202, Average loss:3.943399\n",
      "epoch:203, Average loss:3.969064\n",
      "epoch:204, Average loss:3.918286\n",
      "epoch:205, Average loss:4.072715\n",
      "epoch:206, Average loss:3.950033\n",
      "epoch:207, Average loss:3.876711\n",
      "epoch:208, Average loss:3.940319\n",
      "epoch:209, Average loss:3.865631\n",
      "epoch:210, Average loss:3.914841\n",
      "epoch:211, Average loss:3.861768\n",
      "epoch:212, Average loss:3.864606\n",
      "epoch:213, Average loss:3.894479\n",
      "epoch:214, Average loss:3.818132\n",
      "epoch:215, Average loss:3.844365\n",
      "epoch:216, Average loss:3.889924\n",
      "epoch:217, Average loss:3.82152\n",
      "epoch:218, Average loss:3.750142\n",
      "epoch:219, Average loss:3.852371\n",
      "epoch:220, Average loss:3.804798\n",
      "epoch:221, Average loss:3.765716\n",
      "epoch:222, Average loss:3.843334\n",
      "epoch:223, Average loss:3.746066\n",
      "epoch:224, Average loss:3.749154\n",
      "epoch:225, Average loss:3.73687\n",
      "epoch:226, Average loss:3.715301\n",
      "epoch:227, Average loss:3.785915\n",
      "epoch:228, Average loss:3.702468\n",
      "epoch:229, Average loss:3.711\n",
      "epoch:230, Average loss:3.758248\n",
      "epoch:231, Average loss:3.717731\n",
      "epoch:232, Average loss:3.695058\n",
      "epoch:233, Average loss:3.705455\n",
      "epoch:234, Average loss:3.690593\n",
      "epoch:235, Average loss:3.736004\n",
      "epoch:236, Average loss:3.742233\n",
      "epoch:237, Average loss:3.630962\n",
      "epoch:238, Average loss:3.728902\n",
      "epoch:239, Average loss:3.614994\n",
      "epoch:240, Average loss:3.660359\n",
      "epoch:241, Average loss:3.661323\n",
      "epoch:242, Average loss:3.678752\n",
      "epoch:243, Average loss:3.618584\n",
      "epoch:244, Average loss:3.595957\n",
      "epoch:245, Average loss:3.656393\n",
      "epoch:246, Average loss:3.582004\n",
      "epoch:247, Average loss:3.585082\n",
      "epoch:248, Average loss:3.578777\n",
      "epoch:249, Average loss:3.60887\n",
      "epoch:250, Average loss:3.544529\n",
      "epoch:251, Average loss:3.508821\n",
      "epoch:252, Average loss:3.598564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:253, Average loss:3.536277\n",
      "epoch:254, Average loss:3.611324\n",
      "epoch:255, Average loss:3.625783\n",
      "epoch:256, Average loss:3.520257\n",
      "epoch:257, Average loss:3.634608\n",
      "epoch:258, Average loss:3.579152\n",
      "epoch:259, Average loss:3.566695\n",
      "epoch:260, Average loss:3.531519\n",
      "epoch:261, Average loss:3.497372\n",
      "epoch:262, Average loss:3.523329\n",
      "epoch:263, Average loss:3.452744\n",
      "epoch:264, Average loss:3.496967\n",
      "epoch:265, Average loss:3.485909\n",
      "epoch:266, Average loss:3.505375\n",
      "epoch:267, Average loss:3.49231\n",
      "epoch:268, Average loss:3.567245\n",
      "epoch:269, Average loss:3.45723\n",
      "epoch:270, Average loss:3.503774\n",
      "epoch:271, Average loss:3.420156\n",
      "epoch:272, Average loss:3.492\n",
      "epoch:273, Average loss:3.461158\n",
      "epoch:274, Average loss:3.519635\n",
      "epoch:275, Average loss:3.473402\n",
      "epoch:276, Average loss:3.456997\n",
      "epoch:277, Average loss:3.413737\n",
      "epoch:278, Average loss:3.47703\n",
      "epoch:279, Average loss:3.443362\n",
      "epoch:280, Average loss:3.42652\n",
      "epoch:281, Average loss:3.484167\n",
      "epoch:282, Average loss:3.458219\n",
      "epoch:283, Average loss:3.407167\n",
      "epoch:284, Average loss:3.388821\n",
      "epoch:285, Average loss:3.445178\n",
      "epoch:286, Average loss:3.344531\n",
      "epoch:287, Average loss:3.384617\n",
      "epoch:288, Average loss:3.486928\n",
      "epoch:289, Average loss:3.366189\n",
      "epoch:290, Average loss:3.369518\n",
      "epoch:291, Average loss:3.428343\n",
      "epoch:292, Average loss:3.374103\n",
      "epoch:293, Average loss:3.352179\n",
      "epoch:294, Average loss:3.357285\n",
      "epoch:295, Average loss:3.406256\n",
      "epoch:296, Average loss:3.357425\n",
      "epoch:297, Average loss:3.382685\n",
      "epoch:298, Average loss:3.329738\n",
      "epoch:299, Average loss:3.360991\n",
      "epoch:300, Average loss:3.364358\n",
      "epoch:301, Average loss:3.350125\n",
      "epoch:302, Average loss:3.32481\n",
      "epoch:303, Average loss:3.337262\n",
      "epoch:304, Average loss:3.414717\n",
      "epoch:305, Average loss:3.381495\n",
      "epoch:306, Average loss:3.313104\n",
      "epoch:307, Average loss:3.272869\n",
      "epoch:308, Average loss:3.290284\n",
      "epoch:309, Average loss:3.325552\n",
      "epoch:310, Average loss:3.313572\n",
      "epoch:311, Average loss:3.306492\n",
      "epoch:312, Average loss:3.265221\n",
      "epoch:313, Average loss:3.342609\n",
      "epoch:314, Average loss:3.322856\n",
      "epoch:315, Average loss:3.267287\n",
      "epoch:316, Average loss:3.29689\n",
      "epoch:317, Average loss:3.277843\n",
      "epoch:318, Average loss:3.297327\n",
      "epoch:319, Average loss:3.274695\n",
      "epoch:320, Average loss:3.362416\n",
      "epoch:321, Average loss:3.27156\n",
      "epoch:322, Average loss:3.301812\n",
      "epoch:323, Average loss:3.193748\n",
      "epoch:324, Average loss:3.255576\n",
      "epoch:325, Average loss:3.284609\n",
      "epoch:326, Average loss:3.276352\n",
      "epoch:327, Average loss:3.284\n",
      "epoch:328, Average loss:3.200723\n",
      "epoch:329, Average loss:3.240364\n",
      "epoch:330, Average loss:3.255396\n",
      "epoch:331, Average loss:3.235069\n",
      "epoch:332, Average loss:3.199721\n",
      "epoch:333, Average loss:3.273792\n",
      "epoch:334, Average loss:3.227881\n",
      "epoch:335, Average loss:3.308973\n",
      "epoch:336, Average loss:3.276165\n",
      "epoch:337, Average loss:3.241988\n",
      "epoch:338, Average loss:3.231083\n",
      "epoch:339, Average loss:3.221226\n",
      "epoch:340, Average loss:3.251933\n",
      "epoch:341, Average loss:3.161686\n",
      "epoch:342, Average loss:3.240436\n",
      "epoch:343, Average loss:3.189991\n",
      "epoch:344, Average loss:3.185319\n",
      "epoch:345, Average loss:3.168538\n",
      "epoch:346, Average loss:3.25183\n",
      "epoch:347, Average loss:3.181069\n",
      "epoch:348, Average loss:3.232777\n",
      "epoch:349, Average loss:3.245797\n",
      "epoch:350, Average loss:3.186255\n",
      "epoch:351, Average loss:3.239178\n",
      "epoch:352, Average loss:3.208261\n",
      "epoch:353, Average loss:3.164011\n",
      "epoch:354, Average loss:3.221466\n",
      "epoch:355, Average loss:3.127037\n",
      "epoch:356, Average loss:3.208605\n",
      "epoch:357, Average loss:3.214411\n",
      "epoch:358, Average loss:3.180392\n",
      "epoch:359, Average loss:3.237889\n",
      "epoch:360, Average loss:3.127423\n",
      "epoch:361, Average loss:3.231563\n",
      "epoch:362, Average loss:3.201662\n",
      "epoch:363, Average loss:3.140956\n",
      "epoch:364, Average loss:3.149911\n",
      "epoch:365, Average loss:3.17635\n",
      "epoch:366, Average loss:3.150877\n",
      "epoch:367, Average loss:3.134097\n",
      "epoch:368, Average loss:3.106664\n",
      "epoch:369, Average loss:3.082955\n",
      "epoch:370, Average loss:3.166663\n",
      "epoch:371, Average loss:3.142399\n",
      "epoch:372, Average loss:3.181345\n",
      "epoch:373, Average loss:3.212625\n",
      "epoch:374, Average loss:3.111707\n",
      "epoch:375, Average loss:3.213044\n",
      "epoch:376, Average loss:3.175622\n",
      "epoch:377, Average loss:3.17501\n",
      "epoch:378, Average loss:3.145076\n",
      "epoch:379, Average loss:3.11199\n",
      "epoch:380, Average loss:3.137666\n",
      "epoch:381, Average loss:3.127091\n",
      "epoch:382, Average loss:3.171613\n",
      "epoch:383, Average loss:3.134981\n",
      "epoch:384, Average loss:3.118215\n",
      "epoch:385, Average loss:3.197299\n",
      "epoch:386, Average loss:3.081019\n",
      "epoch:387, Average loss:3.112984\n",
      "epoch:388, Average loss:3.102415\n",
      "epoch:389, Average loss:3.051209\n",
      "epoch:390, Average loss:3.12379\n",
      "epoch:391, Average loss:3.144125\n",
      "epoch:392, Average loss:3.068328\n",
      "epoch:393, Average loss:3.109061\n",
      "epoch:394, Average loss:3.113458\n",
      "epoch:395, Average loss:3.119666\n",
      "epoch:396, Average loss:3.171471\n",
      "epoch:397, Average loss:3.10115\n",
      "epoch:398, Average loss:3.151076\n",
      "epoch:399, Average loss:3.131943\n",
      "epoch:400, Average loss:3.101527\n",
      "epoch:401, Average loss:3.011571\n",
      "epoch:402, Average loss:3.079656\n",
      "epoch:403, Average loss:3.11691\n",
      "epoch:404, Average loss:3.02618\n",
      "epoch:405, Average loss:3.077609\n",
      "epoch:406, Average loss:3.074096\n",
      "epoch:407, Average loss:3.091228\n",
      "epoch:408, Average loss:3.04348\n",
      "epoch:409, Average loss:3.056682\n",
      "epoch:410, Average loss:3.064404\n",
      "epoch:411, Average loss:3.02642\n",
      "epoch:412, Average loss:3.058092\n",
      "epoch:413, Average loss:3.046422\n",
      "epoch:414, Average loss:3.040249\n",
      "epoch:415, Average loss:3.132186\n",
      "epoch:416, Average loss:2.987027\n",
      "epoch:417, Average loss:3.063686\n",
      "epoch:418, Average loss:3.05494\n",
      "epoch:419, Average loss:3.069288\n",
      "epoch:420, Average loss:3.140663\n",
      "epoch:421, Average loss:3.10729\n",
      "epoch:422, Average loss:3.043978\n",
      "epoch:423, Average loss:3.050773\n",
      "epoch:424, Average loss:3.01799\n",
      "epoch:425, Average loss:3.041568\n",
      "epoch:426, Average loss:3.047446\n",
      "epoch:427, Average loss:3.051318\n",
      "epoch:428, Average loss:3.061048\n",
      "epoch:429, Average loss:2.985158\n",
      "epoch:430, Average loss:3.112687\n",
      "epoch:431, Average loss:3.075061\n",
      "epoch:432, Average loss:3.115646\n",
      "epoch:433, Average loss:3.048289\n",
      "epoch:434, Average loss:3.040909\n",
      "epoch:435, Average loss:3.049965\n",
      "epoch:436, Average loss:3.06243\n",
      "epoch:437, Average loss:3.058552\n",
      "epoch:438, Average loss:3.090698\n",
      "epoch:439, Average loss:3.050046\n",
      "epoch:440, Average loss:3.071201\n",
      "epoch:441, Average loss:3.030808\n",
      "epoch:442, Average loss:2.992877\n",
      "epoch:443, Average loss:3.038927\n",
      "epoch:444, Average loss:3.018977\n",
      "epoch:445, Average loss:3.018178\n",
      "epoch:446, Average loss:2.983799\n",
      "epoch:447, Average loss:3.045381\n",
      "epoch:448, Average loss:3.011571\n",
      "epoch:449, Average loss:2.978991\n",
      "epoch:450, Average loss:3.039536\n",
      "epoch:451, Average loss:3.047692\n",
      "epoch:452, Average loss:2.96346\n",
      "epoch:453, Average loss:3.008914\n",
      "epoch:454, Average loss:3.028269\n",
      "epoch:455, Average loss:2.934949\n",
      "epoch:456, Average loss:3.001388\n",
      "epoch:457, Average loss:2.977409\n",
      "epoch:458, Average loss:3.046667\n",
      "epoch:459, Average loss:3.046292\n",
      "epoch:460, Average loss:3.038292\n",
      "epoch:461, Average loss:3.077897\n",
      "epoch:462, Average loss:3.026715\n",
      "epoch:463, Average loss:2.975233\n",
      "epoch:464, Average loss:2.978272\n",
      "epoch:465, Average loss:3.007626\n",
      "epoch:466, Average loss:3.009156\n",
      "epoch:467, Average loss:3.005333\n",
      "epoch:468, Average loss:2.962449\n",
      "epoch:469, Average loss:2.969759\n",
      "epoch:470, Average loss:2.997263\n",
      "epoch:471, Average loss:3.035711\n",
      "epoch:472, Average loss:2.97193\n",
      "epoch:473, Average loss:2.994067\n",
      "epoch:474, Average loss:3.020313\n",
      "epoch:475, Average loss:2.957415\n",
      "epoch:476, Average loss:2.943696\n",
      "epoch:477, Average loss:2.977229\n",
      "epoch:478, Average loss:2.981914\n",
      "epoch:479, Average loss:2.945964\n",
      "epoch:480, Average loss:2.986139\n",
      "epoch:481, Average loss:3.009021\n",
      "epoch:482, Average loss:2.964547\n",
      "epoch:483, Average loss:2.972499\n",
      "epoch:484, Average loss:2.934929\n",
      "epoch:485, Average loss:2.991594\n",
      "epoch:486, Average loss:2.979274\n",
      "epoch:487, Average loss:2.984972\n",
      "epoch:488, Average loss:2.948818\n",
      "epoch:489, Average loss:2.983513\n",
      "epoch:490, Average loss:2.998831\n",
      "epoch:491, Average loss:2.960912\n",
      "epoch:492, Average loss:2.948219\n",
      "epoch:493, Average loss:2.965115\n",
      "epoch:494, Average loss:2.934999\n",
      "epoch:495, Average loss:2.917485\n",
      "epoch:496, Average loss:2.949967\n",
      "epoch:497, Average loss:2.918906\n",
      "epoch:498, Average loss:2.986207\n",
      "epoch:499, Average loss:2.938279\n",
      "epoch:500, Average loss:2.944871\n",
      "epoch:501, Average loss:2.952551\n",
      "epoch:502, Average loss:2.930201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:503, Average loss:2.969147\n",
      "epoch:504, Average loss:2.92428\n",
      "epoch:505, Average loss:2.936592\n",
      "epoch:506, Average loss:2.977297\n",
      "epoch:507, Average loss:2.95936\n",
      "epoch:508, Average loss:2.978122\n",
      "epoch:509, Average loss:2.908093\n",
      "epoch:510, Average loss:2.932312\n",
      "epoch:511, Average loss:2.939285\n",
      "epoch:512, Average loss:2.964261\n",
      "epoch:513, Average loss:2.876209\n",
      "epoch:514, Average loss:2.9291\n",
      "epoch:515, Average loss:2.943052\n",
      "epoch:516, Average loss:2.922459\n",
      "epoch:517, Average loss:2.903786\n",
      "epoch:518, Average loss:2.937923\n",
      "epoch:519, Average loss:2.951805\n",
      "epoch:520, Average loss:2.875827\n",
      "epoch:521, Average loss:2.936041\n",
      "epoch:522, Average loss:2.877187\n",
      "epoch:523, Average loss:2.87695\n",
      "epoch:524, Average loss:2.861285\n",
      "epoch:525, Average loss:2.905636\n",
      "epoch:526, Average loss:2.916723\n",
      "epoch:527, Average loss:2.902006\n",
      "epoch:528, Average loss:2.920549\n",
      "epoch:529, Average loss:2.899446\n",
      "epoch:530, Average loss:2.870859\n",
      "epoch:531, Average loss:2.899411\n",
      "epoch:532, Average loss:2.935432\n",
      "epoch:533, Average loss:2.89778\n",
      "epoch:534, Average loss:2.875364\n",
      "epoch:535, Average loss:2.875912\n",
      "epoch:536, Average loss:2.956436\n",
      "epoch:537, Average loss:2.981231\n",
      "epoch:538, Average loss:2.837244\n",
      "epoch:539, Average loss:2.858027\n",
      "epoch:540, Average loss:2.909982\n",
      "epoch:541, Average loss:2.828003\n",
      "epoch:542, Average loss:2.816069\n",
      "epoch:543, Average loss:2.886546\n",
      "epoch:544, Average loss:2.909952\n",
      "epoch:545, Average loss:2.893724\n",
      "epoch:546, Average loss:2.869121\n",
      "epoch:547, Average loss:2.859199\n",
      "epoch:548, Average loss:2.891203\n",
      "epoch:549, Average loss:2.835623\n",
      "epoch:550, Average loss:2.882941\n",
      "epoch:551, Average loss:2.941724\n",
      "epoch:552, Average loss:2.83087\n",
      "epoch:553, Average loss:2.852137\n",
      "epoch:554, Average loss:2.854728\n",
      "epoch:555, Average loss:2.830224\n",
      "epoch:556, Average loss:2.839307\n",
      "epoch:557, Average loss:2.823302\n",
      "epoch:558, Average loss:2.962077\n",
      "epoch:559, Average loss:2.859216\n",
      "epoch:560, Average loss:2.822494\n",
      "epoch:561, Average loss:2.885863\n",
      "epoch:562, Average loss:2.854559\n",
      "epoch:563, Average loss:2.834667\n",
      "epoch:564, Average loss:2.905984\n",
      "epoch:565, Average loss:2.840609\n",
      "epoch:566, Average loss:2.861027\n",
      "epoch:567, Average loss:2.817872\n",
      "epoch:568, Average loss:2.896078\n",
      "epoch:569, Average loss:2.818126\n",
      "epoch:570, Average loss:2.916138\n",
      "epoch:571, Average loss:2.86127\n",
      "epoch:572, Average loss:2.896246\n",
      "epoch:573, Average loss:2.859121\n",
      "epoch:574, Average loss:2.839109\n",
      "epoch:575, Average loss:2.841479\n",
      "epoch:576, Average loss:2.853937\n",
      "epoch:577, Average loss:2.85562\n",
      "epoch:578, Average loss:2.851535\n",
      "epoch:579, Average loss:2.825355\n",
      "epoch:580, Average loss:2.788624\n",
      "epoch:581, Average loss:2.866081\n",
      "epoch:582, Average loss:2.898872\n",
      "epoch:583, Average loss:2.852636\n",
      "epoch:584, Average loss:2.820128\n",
      "epoch:585, Average loss:2.804941\n",
      "epoch:586, Average loss:2.875109\n",
      "epoch:587, Average loss:2.900179\n",
      "epoch:588, Average loss:2.839557\n",
      "epoch:589, Average loss:2.815548\n",
      "epoch:590, Average loss:2.787487\n",
      "epoch:591, Average loss:2.755856\n",
      "epoch:592, Average loss:2.863518\n",
      "epoch:593, Average loss:2.86128\n",
      "epoch:594, Average loss:2.732703\n",
      "epoch:595, Average loss:2.87111\n",
      "epoch:596, Average loss:2.880903\n",
      "epoch:597, Average loss:2.814625\n",
      "epoch:598, Average loss:2.869318\n",
      "epoch:599, Average loss:2.86425\n",
      "epoch:600, Average loss:2.841451\n",
      "epoch:601, Average loss:2.818038\n",
      "epoch:602, Average loss:2.852329\n",
      "epoch:603, Average loss:2.835214\n",
      "epoch:604, Average loss:2.865736\n",
      "epoch:605, Average loss:2.832643\n",
      "epoch:606, Average loss:2.842552\n",
      "epoch:607, Average loss:2.788028\n",
      "epoch:608, Average loss:2.791819\n",
      "epoch:609, Average loss:2.792958\n",
      "epoch:610, Average loss:2.772348\n",
      "epoch:611, Average loss:2.797965\n",
      "epoch:612, Average loss:2.862278\n",
      "epoch:613, Average loss:2.788216\n",
      "epoch:614, Average loss:2.798338\n",
      "epoch:615, Average loss:2.7932\n",
      "epoch:616, Average loss:2.834606\n",
      "epoch:617, Average loss:2.771288\n",
      "epoch:618, Average loss:2.80647\n",
      "epoch:619, Average loss:2.834041\n",
      "epoch:620, Average loss:2.802802\n",
      "epoch:621, Average loss:2.798954\n",
      "epoch:622, Average loss:2.74179\n",
      "epoch:623, Average loss:2.793517\n",
      "epoch:624, Average loss:2.796143\n",
      "epoch:625, Average loss:2.776129\n",
      "epoch:626, Average loss:2.811922\n",
      "epoch:627, Average loss:2.766109\n",
      "epoch:628, Average loss:2.756776\n",
      "epoch:629, Average loss:2.82895\n",
      "epoch:630, Average loss:2.767837\n",
      "epoch:631, Average loss:2.796338\n",
      "epoch:632, Average loss:2.789714\n",
      "epoch:633, Average loss:2.762904\n",
      "epoch:634, Average loss:2.809\n",
      "epoch:635, Average loss:2.716758\n",
      "epoch:636, Average loss:2.854408\n",
      "epoch:637, Average loss:2.816993\n",
      "epoch:638, Average loss:2.759105\n",
      "epoch:639, Average loss:2.775357\n",
      "epoch:640, Average loss:2.738757\n",
      "epoch:641, Average loss:2.775309\n",
      "epoch:642, Average loss:2.727057\n",
      "epoch:643, Average loss:2.788503\n",
      "epoch:644, Average loss:2.735275\n",
      "epoch:645, Average loss:2.795408\n",
      "epoch:646, Average loss:2.715427\n",
      "epoch:647, Average loss:2.738304\n",
      "epoch:648, Average loss:2.758873\n",
      "epoch:649, Average loss:2.749785\n",
      "epoch:650, Average loss:2.802855\n",
      "epoch:651, Average loss:2.805038\n",
      "epoch:652, Average loss:2.790889\n",
      "epoch:653, Average loss:2.856515\n",
      "epoch:654, Average loss:2.819637\n",
      "epoch:655, Average loss:2.780338\n",
      "epoch:656, Average loss:2.74261\n",
      "epoch:657, Average loss:2.773433\n",
      "epoch:658, Average loss:2.739132\n",
      "epoch:659, Average loss:2.831384\n",
      "epoch:660, Average loss:2.79739\n",
      "epoch:661, Average loss:2.746163\n",
      "epoch:662, Average loss:2.715182\n",
      "epoch:663, Average loss:2.764456\n",
      "epoch:664, Average loss:2.732565\n",
      "epoch:665, Average loss:2.782422\n",
      "epoch:666, Average loss:2.709237\n",
      "epoch:667, Average loss:2.713433\n",
      "epoch:668, Average loss:2.846155\n",
      "epoch:669, Average loss:2.710747\n",
      "epoch:670, Average loss:2.668223\n",
      "epoch:671, Average loss:2.813648\n",
      "epoch:672, Average loss:2.752386\n",
      "epoch:673, Average loss:2.792295\n",
      "epoch:674, Average loss:2.688215\n",
      "epoch:675, Average loss:2.679232\n",
      "epoch:676, Average loss:2.738794\n",
      "epoch:677, Average loss:2.724259\n",
      "epoch:678, Average loss:2.715785\n",
      "epoch:679, Average loss:2.750009\n",
      "epoch:680, Average loss:2.758456\n",
      "epoch:681, Average loss:2.754371\n",
      "epoch:682, Average loss:2.693262\n",
      "epoch:683, Average loss:2.701773\n",
      "epoch:684, Average loss:2.71381\n",
      "epoch:685, Average loss:2.66142\n",
      "epoch:686, Average loss:2.830396\n",
      "epoch:687, Average loss:2.744942\n",
      "epoch:688, Average loss:2.671172\n",
      "epoch:689, Average loss:2.699849\n",
      "epoch:690, Average loss:2.698272\n",
      "epoch:691, Average loss:2.701363\n",
      "epoch:692, Average loss:2.785774\n",
      "epoch:693, Average loss:2.723237\n",
      "epoch:694, Average loss:2.783543\n",
      "epoch:695, Average loss:2.7852\n",
      "epoch:696, Average loss:2.771507\n",
      "epoch:697, Average loss:2.685937\n",
      "epoch:698, Average loss:2.761894\n",
      "epoch:699, Average loss:2.739599\n",
      "epoch:700, Average loss:2.716326\n",
      "epoch:701, Average loss:2.721891\n",
      "epoch:702, Average loss:2.714948\n",
      "epoch:703, Average loss:2.70077\n",
      "epoch:704, Average loss:2.714533\n",
      "epoch:705, Average loss:2.767497\n",
      "epoch:706, Average loss:2.739442\n",
      "epoch:707, Average loss:2.742686\n",
      "epoch:708, Average loss:2.654773\n",
      "epoch:709, Average loss:2.732979\n",
      "epoch:710, Average loss:2.742874\n",
      "epoch:711, Average loss:2.747905\n",
      "epoch:712, Average loss:2.76562\n",
      "epoch:713, Average loss:2.669718\n",
      "epoch:714, Average loss:2.71193\n",
      "epoch:715, Average loss:2.667271\n",
      "epoch:716, Average loss:2.662351\n",
      "epoch:717, Average loss:2.774565\n",
      "epoch:718, Average loss:2.654605\n",
      "epoch:719, Average loss:2.709397\n",
      "epoch:720, Average loss:2.688487\n",
      "epoch:721, Average loss:2.702305\n",
      "epoch:722, Average loss:2.681145\n",
      "epoch:723, Average loss:2.742171\n",
      "epoch:724, Average loss:2.711363\n",
      "epoch:725, Average loss:2.717381\n",
      "epoch:726, Average loss:2.728486\n",
      "epoch:727, Average loss:2.670262\n",
      "epoch:728, Average loss:2.712868\n",
      "epoch:729, Average loss:2.72063\n",
      "epoch:730, Average loss:2.676951\n",
      "epoch:731, Average loss:2.68075\n",
      "epoch:732, Average loss:2.679855\n",
      "epoch:733, Average loss:2.685227\n",
      "epoch:734, Average loss:2.74296\n",
      "epoch:735, Average loss:2.698376\n",
      "epoch:736, Average loss:2.696152\n",
      "epoch:737, Average loss:2.701648\n",
      "epoch:738, Average loss:2.667356\n",
      "epoch:739, Average loss:2.670744\n",
      "epoch:740, Average loss:2.637748\n",
      "epoch:741, Average loss:2.671759\n",
      "epoch:742, Average loss:2.657803\n",
      "epoch:743, Average loss:2.725961\n",
      "epoch:744, Average loss:2.704749\n",
      "epoch:745, Average loss:2.670519\n",
      "epoch:746, Average loss:2.693932\n",
      "epoch:747, Average loss:2.69861\n",
      "epoch:748, Average loss:2.663041\n",
      "epoch:749, Average loss:2.709816\n",
      "epoch:750, Average loss:2.712201\n",
      "epoch:751, Average loss:2.684105\n",
      "epoch:752, Average loss:2.674683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:753, Average loss:2.705233\n",
      "epoch:754, Average loss:2.656785\n",
      "epoch:755, Average loss:2.7173\n",
      "epoch:756, Average loss:2.66298\n",
      "epoch:757, Average loss:2.665103\n",
      "epoch:758, Average loss:2.641633\n",
      "epoch:759, Average loss:2.692214\n",
      "epoch:760, Average loss:2.710087\n",
      "epoch:761, Average loss:2.661892\n",
      "epoch:762, Average loss:2.715034\n",
      "epoch:763, Average loss:2.635388\n",
      "epoch:764, Average loss:2.698218\n",
      "epoch:765, Average loss:2.68229\n",
      "epoch:766, Average loss:2.678867\n",
      "epoch:767, Average loss:2.667302\n",
      "epoch:768, Average loss:2.632492\n",
      "epoch:769, Average loss:2.652424\n",
      "epoch:770, Average loss:2.61101\n",
      "epoch:771, Average loss:2.618566\n",
      "epoch:772, Average loss:2.693602\n",
      "epoch:773, Average loss:2.69277\n",
      "epoch:774, Average loss:2.655165\n",
      "epoch:775, Average loss:2.693408\n",
      "epoch:776, Average loss:2.6534\n",
      "epoch:777, Average loss:2.696608\n",
      "epoch:778, Average loss:2.671868\n",
      "epoch:779, Average loss:2.65923\n",
      "epoch:780, Average loss:2.70295\n",
      "epoch:781, Average loss:2.669515\n",
      "epoch:782, Average loss:2.632166\n",
      "epoch:783, Average loss:2.66698\n",
      "epoch:784, Average loss:2.642086\n",
      "epoch:785, Average loss:2.707409\n",
      "epoch:786, Average loss:2.678982\n",
      "epoch:787, Average loss:2.644338\n",
      "epoch:788, Average loss:2.651861\n",
      "epoch:789, Average loss:2.644165\n",
      "epoch:790, Average loss:2.604829\n",
      "epoch:791, Average loss:2.667306\n",
      "epoch:792, Average loss:2.649435\n",
      "epoch:793, Average loss:2.632158\n",
      "epoch:794, Average loss:2.652405\n",
      "epoch:795, Average loss:2.650812\n",
      "epoch:796, Average loss:2.598672\n",
      "epoch:797, Average loss:2.668678\n",
      "epoch:798, Average loss:2.602991\n",
      "epoch:799, Average loss:2.639044\n",
      "epoch:800, Average loss:2.692038\n",
      "epoch:801, Average loss:2.637726\n",
      "epoch:802, Average loss:2.655762\n",
      "epoch:803, Average loss:2.633021\n",
      "epoch:804, Average loss:2.631406\n",
      "epoch:805, Average loss:2.591744\n",
      "epoch:806, Average loss:2.640387\n",
      "epoch:807, Average loss:2.630817\n",
      "epoch:808, Average loss:2.613913\n",
      "epoch:809, Average loss:2.672147\n",
      "epoch:810, Average loss:2.598356\n",
      "epoch:811, Average loss:2.643003\n",
      "epoch:812, Average loss:2.654109\n",
      "epoch:813, Average loss:2.601044\n",
      "epoch:814, Average loss:2.624244\n",
      "epoch:815, Average loss:2.629551\n",
      "epoch:816, Average loss:2.563536\n",
      "epoch:817, Average loss:2.628747\n",
      "epoch:818, Average loss:2.57688\n",
      "epoch:819, Average loss:2.629404\n",
      "epoch:820, Average loss:2.573299\n",
      "epoch:821, Average loss:2.651478\n",
      "epoch:822, Average loss:2.624687\n",
      "epoch:823, Average loss:2.620989\n",
      "epoch:824, Average loss:2.597056\n",
      "epoch:825, Average loss:2.670686\n",
      "epoch:826, Average loss:2.622146\n",
      "epoch:827, Average loss:2.615488\n",
      "epoch:828, Average loss:2.682316\n",
      "epoch:829, Average loss:2.560165\n",
      "epoch:830, Average loss:2.657974\n",
      "epoch:831, Average loss:2.615595\n",
      "epoch:832, Average loss:2.648593\n",
      "epoch:833, Average loss:2.616367\n",
      "epoch:834, Average loss:2.665808\n",
      "epoch:835, Average loss:2.601368\n",
      "epoch:836, Average loss:2.592122\n",
      "epoch:837, Average loss:2.601363\n",
      "epoch:838, Average loss:2.670357\n",
      "epoch:839, Average loss:2.613199\n",
      "epoch:840, Average loss:2.65518\n",
      "epoch:841, Average loss:2.588972\n",
      "epoch:842, Average loss:2.617727\n",
      "epoch:843, Average loss:2.657875\n",
      "epoch:844, Average loss:2.589428\n",
      "epoch:845, Average loss:2.617923\n",
      "epoch:846, Average loss:2.62501\n",
      "epoch:847, Average loss:2.651488\n",
      "epoch:848, Average loss:2.604476\n",
      "epoch:849, Average loss:2.616095\n",
      "epoch:850, Average loss:2.551079\n",
      "epoch:851, Average loss:2.63559\n",
      "epoch:852, Average loss:2.639697\n",
      "epoch:853, Average loss:2.601455\n",
      "epoch:854, Average loss:2.578249\n",
      "epoch:855, Average loss:2.576207\n",
      "epoch:856, Average loss:2.588948\n",
      "epoch:857, Average loss:2.651787\n",
      "epoch:858, Average loss:2.615739\n",
      "epoch:859, Average loss:2.634333\n",
      "epoch:860, Average loss:2.602255\n",
      "epoch:861, Average loss:2.65946\n",
      "epoch:862, Average loss:2.593829\n",
      "epoch:863, Average loss:2.610674\n",
      "epoch:864, Average loss:2.606569\n",
      "epoch:865, Average loss:2.566203\n",
      "epoch:866, Average loss:2.616426\n",
      "epoch:867, Average loss:2.591946\n",
      "epoch:868, Average loss:2.563428\n",
      "epoch:869, Average loss:2.532833\n",
      "epoch:870, Average loss:2.56978\n",
      "epoch:871, Average loss:2.600083\n",
      "epoch:872, Average loss:2.538531\n",
      "epoch:873, Average loss:2.510916\n",
      "epoch:874, Average loss:2.567805\n",
      "epoch:875, Average loss:2.589353\n",
      "epoch:876, Average loss:2.579763\n",
      "epoch:877, Average loss:2.529788\n",
      "epoch:878, Average loss:2.595996\n",
      "epoch:879, Average loss:2.578038\n",
      "epoch:880, Average loss:2.577186\n",
      "epoch:881, Average loss:2.559617\n",
      "epoch:882, Average loss:2.556745\n",
      "epoch:883, Average loss:2.586395\n",
      "epoch:884, Average loss:2.571564\n",
      "epoch:885, Average loss:2.588309\n",
      "epoch:886, Average loss:2.599958\n",
      "epoch:887, Average loss:2.575846\n",
      "epoch:888, Average loss:2.521358\n",
      "epoch:889, Average loss:2.539303\n",
      "epoch:890, Average loss:2.574752\n",
      "epoch:891, Average loss:2.571508\n",
      "epoch:892, Average loss:2.562876\n",
      "epoch:893, Average loss:2.539112\n",
      "epoch:894, Average loss:2.590125\n",
      "epoch:895, Average loss:2.565749\n",
      "epoch:896, Average loss:2.583553\n",
      "epoch:897, Average loss:2.536703\n",
      "epoch:898, Average loss:2.54155\n",
      "epoch:899, Average loss:2.57325\n",
      "epoch:900, Average loss:2.513775\n",
      "epoch:901, Average loss:2.503056\n",
      "epoch:902, Average loss:2.617018\n",
      "epoch:903, Average loss:2.540891\n",
      "epoch:904, Average loss:2.595706\n",
      "epoch:905, Average loss:2.506801\n",
      "epoch:906, Average loss:2.623697\n",
      "epoch:907, Average loss:2.569132\n",
      "epoch:908, Average loss:2.516874\n",
      "epoch:909, Average loss:2.53204\n",
      "epoch:910, Average loss:2.559457\n",
      "epoch:911, Average loss:2.55803\n",
      "epoch:912, Average loss:2.589164\n",
      "epoch:913, Average loss:2.648874\n",
      "epoch:914, Average loss:2.523849\n",
      "epoch:915, Average loss:2.584176\n",
      "epoch:916, Average loss:2.510956\n",
      "epoch:917, Average loss:2.525778\n",
      "epoch:918, Average loss:2.561811\n",
      "epoch:919, Average loss:2.46853\n",
      "epoch:920, Average loss:2.538166\n",
      "epoch:921, Average loss:2.540741\n",
      "epoch:922, Average loss:2.580768\n",
      "epoch:923, Average loss:2.522642\n",
      "epoch:924, Average loss:2.551885\n",
      "epoch:925, Average loss:2.586469\n",
      "epoch:926, Average loss:2.550186\n",
      "epoch:927, Average loss:2.569616\n",
      "epoch:928, Average loss:2.501561\n",
      "epoch:929, Average loss:2.508795\n",
      "epoch:930, Average loss:2.621653\n",
      "epoch:931, Average loss:2.522685\n",
      "epoch:932, Average loss:2.522275\n",
      "epoch:933, Average loss:2.588847\n",
      "epoch:934, Average loss:2.520107\n",
      "epoch:935, Average loss:2.533189\n",
      "epoch:936, Average loss:2.556395\n",
      "epoch:937, Average loss:2.567735\n",
      "epoch:938, Average loss:2.564288\n",
      "epoch:939, Average loss:2.500735\n",
      "epoch:940, Average loss:2.565229\n",
      "epoch:941, Average loss:2.540119\n",
      "epoch:942, Average loss:2.504328\n",
      "epoch:943, Average loss:2.556633\n",
      "epoch:944, Average loss:2.461571\n",
      "epoch:945, Average loss:2.56207\n",
      "epoch:946, Average loss:2.472727\n",
      "epoch:947, Average loss:2.504734\n",
      "epoch:948, Average loss:2.555226\n",
      "epoch:949, Average loss:2.473974\n",
      "epoch:950, Average loss:2.545756\n",
      "epoch:951, Average loss:2.538812\n",
      "epoch:952, Average loss:2.620337\n",
      "epoch:953, Average loss:2.537388\n",
      "epoch:954, Average loss:2.534522\n",
      "epoch:955, Average loss:2.539578\n",
      "epoch:956, Average loss:2.561595\n",
      "epoch:957, Average loss:2.481948\n",
      "epoch:958, Average loss:2.502399\n",
      "epoch:959, Average loss:2.524743\n",
      "epoch:960, Average loss:2.539088\n",
      "epoch:961, Average loss:2.499627\n",
      "epoch:962, Average loss:2.482026\n",
      "epoch:963, Average loss:2.486677\n",
      "epoch:964, Average loss:2.510889\n",
      "epoch:965, Average loss:2.51598\n",
      "epoch:966, Average loss:2.520382\n",
      "epoch:967, Average loss:2.508379\n",
      "epoch:968, Average loss:2.568113\n",
      "epoch:969, Average loss:2.516289\n",
      "epoch:970, Average loss:2.512706\n",
      "epoch:971, Average loss:2.499671\n",
      "epoch:972, Average loss:2.504479\n",
      "epoch:973, Average loss:2.547001\n",
      "epoch:974, Average loss:2.500261\n",
      "epoch:975, Average loss:2.461815\n",
      "epoch:976, Average loss:2.516999\n",
      "epoch:977, Average loss:2.538228\n",
      "epoch:978, Average loss:2.543041\n",
      "epoch:979, Average loss:2.462165\n",
      "epoch:980, Average loss:2.505691\n",
      "epoch:981, Average loss:2.55319\n",
      "epoch:982, Average loss:2.504071\n",
      "epoch:983, Average loss:2.533133\n",
      "epoch:984, Average loss:2.530206\n",
      "epoch:985, Average loss:2.47714\n",
      "epoch:986, Average loss:2.469206\n",
      "epoch:987, Average loss:2.482933\n",
      "epoch:988, Average loss:2.544646\n",
      "epoch:989, Average loss:2.467253\n",
      "epoch:990, Average loss:2.443846\n",
      "epoch:991, Average loss:2.484556\n",
      "epoch:992, Average loss:2.5264\n",
      "epoch:993, Average loss:2.458152\n",
      "epoch:994, Average loss:2.531246\n",
      "epoch:995, Average loss:2.500192\n",
      "epoch:996, Average loss:2.479541\n",
      "epoch:997, Average loss:2.543984\n",
      "epoch:998, Average loss:2.485473\n",
      "epoch:999, Average loss:2.428802\n",
      "epoch:1000, Average loss:2.53067\n",
      "epoch:1001, Average loss:2.546847\n",
      "epoch:1002, Average loss:2.518615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1003, Average loss:2.464737\n",
      "epoch:1004, Average loss:2.485803\n",
      "epoch:1005, Average loss:2.487614\n",
      "epoch:1006, Average loss:2.566159\n",
      "epoch:1007, Average loss:2.500508\n",
      "epoch:1008, Average loss:2.462932\n",
      "epoch:1009, Average loss:2.520467\n",
      "epoch:1010, Average loss:2.462524\n",
      "epoch:1011, Average loss:2.459479\n",
      "epoch:1012, Average loss:2.506872\n",
      "epoch:1013, Average loss:2.553766\n",
      "epoch:1014, Average loss:2.498054\n",
      "epoch:1015, Average loss:2.476731\n",
      "epoch:1016, Average loss:2.455862\n",
      "epoch:1017, Average loss:2.496322\n",
      "epoch:1018, Average loss:2.487577\n",
      "epoch:1019, Average loss:2.477568\n",
      "epoch:1020, Average loss:2.456278\n",
      "epoch:1021, Average loss:2.471287\n",
      "epoch:1022, Average loss:2.464497\n",
      "epoch:1023, Average loss:2.482422\n",
      "epoch:1024, Average loss:2.482031\n",
      "epoch:1025, Average loss:2.455089\n",
      "epoch:1026, Average loss:2.467936\n",
      "epoch:1027, Average loss:2.493954\n",
      "epoch:1028, Average loss:2.48199\n",
      "epoch:1029, Average loss:2.435489\n",
      "epoch:1030, Average loss:2.461224\n",
      "epoch:1031, Average loss:2.437852\n",
      "epoch:1032, Average loss:2.451866\n",
      "epoch:1033, Average loss:2.540971\n",
      "epoch:1034, Average loss:2.463915\n",
      "epoch:1035, Average loss:2.496594\n",
      "epoch:1036, Average loss:2.467802\n",
      "epoch:1037, Average loss:2.450309\n",
      "epoch:1038, Average loss:2.449694\n",
      "epoch:1039, Average loss:2.504007\n",
      "epoch:1040, Average loss:2.504458\n",
      "epoch:1041, Average loss:2.480687\n",
      "epoch:1042, Average loss:2.488517\n",
      "epoch:1043, Average loss:2.505684\n",
      "epoch:1044, Average loss:2.519099\n",
      "epoch:1045, Average loss:2.516927\n",
      "epoch:1046, Average loss:2.425536\n",
      "epoch:1047, Average loss:2.456798\n",
      "epoch:1048, Average loss:2.431897\n",
      "epoch:1049, Average loss:2.451581\n",
      "epoch:1050, Average loss:2.50223\n",
      "epoch:1051, Average loss:2.408473\n",
      "epoch:1052, Average loss:2.442641\n",
      "epoch:1053, Average loss:2.458474\n",
      "epoch:1054, Average loss:2.493778\n",
      "epoch:1055, Average loss:2.467456\n",
      "epoch:1056, Average loss:2.426915\n",
      "epoch:1057, Average loss:2.490319\n",
      "epoch:1058, Average loss:2.490832\n",
      "epoch:1059, Average loss:2.431046\n",
      "epoch:1060, Average loss:2.436997\n",
      "epoch:1061, Average loss:2.455071\n",
      "epoch:1062, Average loss:2.415462\n",
      "epoch:1063, Average loss:2.45327\n",
      "epoch:1064, Average loss:2.539505\n",
      "epoch:1065, Average loss:2.491312\n",
      "epoch:1066, Average loss:2.435936\n",
      "epoch:1067, Average loss:2.478027\n",
      "epoch:1068, Average loss:2.517955\n",
      "epoch:1069, Average loss:2.43302\n",
      "epoch:1070, Average loss:2.427702\n",
      "epoch:1071, Average loss:2.459147\n",
      "epoch:1072, Average loss:2.444175\n",
      "epoch:1073, Average loss:2.484691\n",
      "epoch:1074, Average loss:2.499073\n",
      "epoch:1075, Average loss:2.44113\n",
      "epoch:1076, Average loss:2.454355\n",
      "epoch:1077, Average loss:2.416144\n",
      "epoch:1078, Average loss:2.435884\n",
      "epoch:1079, Average loss:2.409589\n",
      "epoch:1080, Average loss:2.479571\n",
      "epoch:1081, Average loss:2.463629\n",
      "epoch:1082, Average loss:2.383243\n",
      "epoch:1083, Average loss:2.425794\n",
      "epoch:1084, Average loss:2.482452\n",
      "epoch:1085, Average loss:2.465797\n",
      "epoch:1086, Average loss:2.504673\n",
      "epoch:1087, Average loss:2.476926\n",
      "epoch:1088, Average loss:2.385045\n",
      "epoch:1089, Average loss:2.515156\n",
      "epoch:1090, Average loss:2.42203\n",
      "epoch:1091, Average loss:2.444048\n",
      "epoch:1092, Average loss:2.485695\n",
      "epoch:1093, Average loss:2.406446\n",
      "epoch:1094, Average loss:2.443775\n",
      "epoch:1095, Average loss:2.424235\n",
      "epoch:1096, Average loss:2.41751\n",
      "epoch:1097, Average loss:2.430615\n",
      "epoch:1098, Average loss:2.438506\n",
      "epoch:1099, Average loss:2.428607\n",
      "epoch:1100, Average loss:2.419754\n",
      "epoch:1101, Average loss:2.409895\n",
      "epoch:1102, Average loss:2.469389\n",
      "epoch:1103, Average loss:2.393254\n",
      "epoch:1104, Average loss:2.44954\n",
      "epoch:1105, Average loss:2.401056\n",
      "epoch:1106, Average loss:2.400297\n",
      "epoch:1107, Average loss:2.469979\n",
      "epoch:1108, Average loss:2.414373\n",
      "epoch:1109, Average loss:2.441706\n",
      "epoch:1110, Average loss:2.454334\n",
      "epoch:1111, Average loss:2.420297\n",
      "epoch:1112, Average loss:2.457647\n",
      "epoch:1113, Average loss:2.40623\n",
      "epoch:1114, Average loss:2.39467\n",
      "epoch:1115, Average loss:2.432235\n",
      "epoch:1116, Average loss:2.423323\n",
      "epoch:1117, Average loss:2.435196\n",
      "epoch:1118, Average loss:2.461811\n",
      "epoch:1119, Average loss:2.414574\n",
      "epoch:1120, Average loss:2.46428\n",
      "epoch:1121, Average loss:2.361629\n",
      "epoch:1122, Average loss:2.445235\n",
      "epoch:1123, Average loss:2.445654\n",
      "epoch:1124, Average loss:2.387652\n",
      "epoch:1125, Average loss:2.413021\n",
      "epoch:1126, Average loss:2.411791\n",
      "epoch:1127, Average loss:2.41349\n",
      "epoch:1128, Average loss:2.403861\n",
      "epoch:1129, Average loss:2.417886\n",
      "epoch:1130, Average loss:2.42494\n",
      "epoch:1131, Average loss:2.371667\n",
      "epoch:1132, Average loss:2.460536\n",
      "epoch:1133, Average loss:2.445836\n",
      "epoch:1134, Average loss:2.478974\n",
      "epoch:1135, Average loss:2.399306\n",
      "epoch:1136, Average loss:2.47336\n",
      "epoch:1137, Average loss:2.421507\n",
      "epoch:1138, Average loss:2.436207\n",
      "epoch:1139, Average loss:2.459062\n",
      "epoch:1140, Average loss:2.436848\n",
      "epoch:1141, Average loss:2.392411\n",
      "epoch:1142, Average loss:2.471674\n",
      "epoch:1143, Average loss:2.379868\n",
      "epoch:1144, Average loss:2.355715\n",
      "epoch:1145, Average loss:2.410071\n",
      "epoch:1146, Average loss:2.47253\n",
      "epoch:1147, Average loss:2.423135\n",
      "epoch:1148, Average loss:2.41306\n",
      "epoch:1149, Average loss:2.41929\n",
      "epoch:1150, Average loss:2.385915\n",
      "epoch:1151, Average loss:2.398435\n",
      "epoch:1152, Average loss:2.399207\n",
      "epoch:1153, Average loss:2.36995\n",
      "epoch:1154, Average loss:2.370008\n",
      "epoch:1155, Average loss:2.405155\n",
      "epoch:1156, Average loss:2.378943\n",
      "epoch:1157, Average loss:2.388588\n",
      "epoch:1158, Average loss:2.363055\n",
      "epoch:1159, Average loss:2.378271\n",
      "epoch:1160, Average loss:2.344911\n",
      "epoch:1161, Average loss:2.399037\n",
      "epoch:1162, Average loss:2.352049\n",
      "epoch:1163, Average loss:2.387057\n",
      "epoch:1164, Average loss:2.37894\n",
      "epoch:1165, Average loss:2.401154\n",
      "epoch:1166, Average loss:2.389032\n",
      "epoch:1167, Average loss:2.470544\n",
      "epoch:1168, Average loss:2.46384\n",
      "epoch:1169, Average loss:2.437854\n",
      "epoch:1170, Average loss:2.468092\n",
      "epoch:1171, Average loss:2.359841\n",
      "epoch:1172, Average loss:2.342323\n",
      "epoch:1173, Average loss:2.397988\n",
      "epoch:1174, Average loss:2.363996\n",
      "epoch:1175, Average loss:2.335162\n",
      "epoch:1176, Average loss:2.404166\n",
      "epoch:1177, Average loss:2.375059\n",
      "epoch:1178, Average loss:2.409006\n",
      "epoch:1179, Average loss:2.390422\n",
      "epoch:1180, Average loss:2.29671\n",
      "epoch:1181, Average loss:2.402918\n",
      "epoch:1182, Average loss:2.42698\n",
      "epoch:1183, Average loss:2.369156\n",
      "epoch:1184, Average loss:2.377756\n",
      "epoch:1185, Average loss:2.420319\n",
      "epoch:1186, Average loss:2.413342\n",
      "epoch:1187, Average loss:2.389057\n",
      "epoch:1188, Average loss:2.406411\n",
      "epoch:1189, Average loss:2.333626\n",
      "epoch:1190, Average loss:2.346081\n",
      "epoch:1191, Average loss:2.3928\n",
      "epoch:1192, Average loss:2.386221\n",
      "epoch:1193, Average loss:2.39968\n",
      "epoch:1194, Average loss:2.376633\n",
      "epoch:1195, Average loss:2.373306\n",
      "epoch:1196, Average loss:2.352091\n",
      "epoch:1197, Average loss:2.349503\n",
      "epoch:1198, Average loss:2.380596\n",
      "epoch:1199, Average loss:2.359505\n",
      "epoch:1200, Average loss:2.355267\n",
      "epoch:1201, Average loss:2.387977\n",
      "epoch:1202, Average loss:2.367491\n",
      "epoch:1203, Average loss:2.389844\n",
      "epoch:1204, Average loss:2.353214\n",
      "epoch:1205, Average loss:2.372025\n",
      "epoch:1206, Average loss:2.389388\n",
      "epoch:1207, Average loss:2.33717\n",
      "epoch:1208, Average loss:2.335291\n",
      "epoch:1209, Average loss:2.338892\n",
      "epoch:1210, Average loss:2.381304\n",
      "epoch:1211, Average loss:2.403327\n",
      "epoch:1212, Average loss:2.440548\n",
      "epoch:1213, Average loss:2.345313\n",
      "epoch:1214, Average loss:2.340645\n",
      "epoch:1215, Average loss:2.349461\n",
      "epoch:1216, Average loss:2.34174\n",
      "epoch:1217, Average loss:2.32406\n",
      "epoch:1218, Average loss:2.397128\n",
      "epoch:1219, Average loss:2.347863\n",
      "epoch:1220, Average loss:2.376933\n",
      "epoch:1221, Average loss:2.405284\n",
      "epoch:1222, Average loss:2.395129\n",
      "epoch:1223, Average loss:2.328063\n",
      "epoch:1224, Average loss:2.340176\n",
      "epoch:1225, Average loss:2.380852\n",
      "epoch:1226, Average loss:2.361408\n",
      "epoch:1227, Average loss:2.280796\n",
      "epoch:1228, Average loss:2.311376\n",
      "epoch:1229, Average loss:2.393647\n",
      "epoch:1230, Average loss:2.297658\n",
      "epoch:1231, Average loss:2.355703\n",
      "epoch:1232, Average loss:2.322886\n",
      "epoch:1233, Average loss:2.341925\n",
      "epoch:1234, Average loss:2.360409\n",
      "epoch:1235, Average loss:2.302142\n",
      "epoch:1236, Average loss:2.394791\n",
      "epoch:1237, Average loss:2.337649\n",
      "epoch:1238, Average loss:2.325708\n",
      "epoch:1239, Average loss:2.362417\n",
      "epoch:1240, Average loss:2.43706\n",
      "epoch:1241, Average loss:2.305619\n",
      "epoch:1242, Average loss:2.330318\n",
      "epoch:1243, Average loss:2.36482\n",
      "epoch:1244, Average loss:2.384577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1245, Average loss:2.34594\n",
      "epoch:1246, Average loss:2.279485\n",
      "epoch:1247, Average loss:2.346515\n",
      "epoch:1248, Average loss:2.337735\n",
      "epoch:1249, Average loss:2.316196\n",
      "epoch:1250, Average loss:2.357784\n",
      "epoch:1251, Average loss:2.368098\n",
      "epoch:1252, Average loss:2.349739\n",
      "epoch:1253, Average loss:2.398829\n",
      "epoch:1254, Average loss:2.315246\n",
      "epoch:1255, Average loss:2.38472\n",
      "epoch:1256, Average loss:2.403771\n",
      "epoch:1257, Average loss:2.345105\n",
      "epoch:1258, Average loss:2.268143\n",
      "epoch:1259, Average loss:2.361316\n",
      "epoch:1260, Average loss:2.286118\n",
      "epoch:1261, Average loss:2.292104\n",
      "epoch:1262, Average loss:2.380831\n",
      "epoch:1263, Average loss:2.325817\n",
      "epoch:1264, Average loss:2.278586\n",
      "epoch:1265, Average loss:2.369328\n",
      "epoch:1266, Average loss:2.343569\n",
      "epoch:1267, Average loss:2.336793\n",
      "epoch:1268, Average loss:2.330253\n",
      "epoch:1269, Average loss:2.325107\n",
      "epoch:1270, Average loss:2.373854\n",
      "epoch:1271, Average loss:2.320316\n",
      "epoch:1272, Average loss:2.338451\n",
      "epoch:1273, Average loss:2.352674\n",
      "epoch:1274, Average loss:2.358466\n",
      "epoch:1275, Average loss:2.30951\n",
      "epoch:1276, Average loss:2.377042\n",
      "epoch:1277, Average loss:2.306696\n",
      "epoch:1278, Average loss:2.352357\n",
      "epoch:1279, Average loss:2.29919\n",
      "epoch:1280, Average loss:2.292466\n",
      "epoch:1281, Average loss:2.288918\n",
      "epoch:1282, Average loss:2.320086\n",
      "epoch:1283, Average loss:2.333339\n",
      "epoch:1284, Average loss:2.340768\n",
      "epoch:1285, Average loss:2.286789\n",
      "epoch:1286, Average loss:2.348683\n",
      "epoch:1287, Average loss:2.300276\n",
      "epoch:1288, Average loss:2.349467\n",
      "epoch:1289, Average loss:2.327273\n",
      "epoch:1290, Average loss:2.294787\n",
      "epoch:1291, Average loss:2.334153\n",
      "epoch:1292, Average loss:2.303466\n",
      "epoch:1293, Average loss:2.282309\n",
      "epoch:1294, Average loss:2.331293\n",
      "epoch:1295, Average loss:2.304771\n",
      "epoch:1296, Average loss:2.297667\n",
      "epoch:1297, Average loss:2.308641\n",
      "epoch:1298, Average loss:2.312466\n",
      "epoch:1299, Average loss:2.373592\n",
      "epoch:1300, Average loss:2.316685\n",
      "epoch:1301, Average loss:2.322984\n",
      "epoch:1302, Average loss:2.329134\n",
      "epoch:1303, Average loss:2.286795\n",
      "epoch:1304, Average loss:2.327428\n",
      "epoch:1305, Average loss:2.315898\n",
      "epoch:1306, Average loss:2.306541\n",
      "epoch:1307, Average loss:2.269391\n",
      "epoch:1308, Average loss:2.302394\n",
      "epoch:1309, Average loss:2.290847\n",
      "epoch:1310, Average loss:2.274449\n",
      "epoch:1311, Average loss:2.243669\n",
      "epoch:1312, Average loss:2.332866\n",
      "epoch:1313, Average loss:2.265707\n",
      "epoch:1314, Average loss:2.329237\n",
      "epoch:1315, Average loss:2.263321\n",
      "epoch:1316, Average loss:2.306939\n",
      "epoch:1317, Average loss:2.338044\n",
      "epoch:1318, Average loss:2.253984\n",
      "epoch:1319, Average loss:2.236181\n",
      "epoch:1320, Average loss:2.317178\n",
      "epoch:1321, Average loss:2.261686\n",
      "epoch:1322, Average loss:2.316255\n",
      "epoch:1323, Average loss:2.314257\n",
      "epoch:1324, Average loss:2.294989\n",
      "epoch:1325, Average loss:2.287863\n",
      "epoch:1326, Average loss:2.27363\n",
      "epoch:1327, Average loss:2.292213\n",
      "epoch:1328, Average loss:2.295401\n",
      "epoch:1329, Average loss:2.282042\n",
      "epoch:1330, Average loss:2.283104\n",
      "epoch:1331, Average loss:2.29484\n",
      "epoch:1332, Average loss:2.288137\n",
      "epoch:1333, Average loss:2.28572\n",
      "epoch:1334, Average loss:2.306708\n",
      "epoch:1335, Average loss:2.287818\n",
      "epoch:1336, Average loss:2.243445\n",
      "epoch:1337, Average loss:2.325532\n",
      "epoch:1338, Average loss:2.251971\n",
      "epoch:1339, Average loss:2.311639\n",
      "epoch:1340, Average loss:2.285067\n",
      "epoch:1341, Average loss:2.306649\n",
      "epoch:1342, Average loss:2.319108\n",
      "epoch:1343, Average loss:2.249838\n",
      "epoch:1344, Average loss:2.275953\n",
      "epoch:1345, Average loss:2.26623\n",
      "epoch:1346, Average loss:2.314523\n",
      "epoch:1347, Average loss:2.233345\n",
      "epoch:1348, Average loss:2.304481\n",
      "epoch:1349, Average loss:2.285858\n",
      "epoch:1350, Average loss:2.268585\n",
      "epoch:1351, Average loss:2.296153\n",
      "epoch:1352, Average loss:2.242546\n",
      "epoch:1353, Average loss:2.298\n",
      "epoch:1354, Average loss:2.230819\n",
      "epoch:1355, Average loss:2.287148\n",
      "epoch:1356, Average loss:2.305864\n",
      "epoch:1357, Average loss:2.346653\n",
      "epoch:1358, Average loss:2.257548\n",
      "epoch:1359, Average loss:2.265504\n",
      "epoch:1360, Average loss:2.296119\n",
      "epoch:1361, Average loss:2.294328\n",
      "epoch:1362, Average loss:2.279479\n",
      "epoch:1363, Average loss:2.282642\n",
      "epoch:1364, Average loss:2.263815\n",
      "epoch:1365, Average loss:2.273997\n",
      "epoch:1366, Average loss:2.234174\n",
      "epoch:1367, Average loss:2.243161\n",
      "epoch:1368, Average loss:2.235275\n",
      "epoch:1369, Average loss:2.275012\n",
      "epoch:1370, Average loss:2.240673\n",
      "epoch:1371, Average loss:2.29631\n",
      "epoch:1372, Average loss:2.251821\n",
      "epoch:1373, Average loss:2.198748\n",
      "epoch:1374, Average loss:2.234573\n",
      "epoch:1375, Average loss:2.263162\n",
      "epoch:1376, Average loss:2.241447\n",
      "epoch:1377, Average loss:2.271779\n",
      "epoch:1378, Average loss:2.245078\n",
      "epoch:1379, Average loss:2.288553\n",
      "epoch:1380, Average loss:2.250685\n",
      "epoch:1381, Average loss:2.259776\n",
      "epoch:1382, Average loss:2.278598\n",
      "epoch:1383, Average loss:2.261531\n",
      "epoch:1384, Average loss:2.257539\n",
      "epoch:1385, Average loss:2.268844\n",
      "epoch:1386, Average loss:2.250386\n",
      "epoch:1387, Average loss:2.256326\n",
      "epoch:1388, Average loss:2.258443\n",
      "epoch:1389, Average loss:2.328397\n",
      "epoch:1390, Average loss:2.225666\n",
      "epoch:1391, Average loss:2.261991\n",
      "epoch:1392, Average loss:2.269002\n",
      "epoch:1393, Average loss:2.230875\n",
      "epoch:1394, Average loss:2.269789\n",
      "epoch:1395, Average loss:2.257637\n",
      "epoch:1396, Average loss:2.274081\n",
      "epoch:1397, Average loss:2.239473\n",
      "epoch:1398, Average loss:2.20743\n",
      "epoch:1399, Average loss:2.24165\n",
      "epoch:1400, Average loss:2.247002\n",
      "epoch:1401, Average loss:2.25942\n",
      "epoch:1402, Average loss:2.307896\n",
      "epoch:1403, Average loss:2.242357\n",
      "epoch:1404, Average loss:2.268149\n",
      "epoch:1405, Average loss:2.279717\n",
      "epoch:1406, Average loss:2.213241\n",
      "epoch:1407, Average loss:2.237553\n",
      "epoch:1408, Average loss:2.200339\n",
      "epoch:1409, Average loss:2.223533\n",
      "epoch:1410, Average loss:2.259679\n",
      "epoch:1411, Average loss:2.233317\n",
      "epoch:1412, Average loss:2.203391\n",
      "epoch:1413, Average loss:2.211303\n",
      "epoch:1414, Average loss:2.258776\n",
      "epoch:1415, Average loss:2.202347\n",
      "epoch:1416, Average loss:2.28875\n",
      "epoch:1417, Average loss:2.17716\n",
      "epoch:1418, Average loss:2.260606\n",
      "epoch:1419, Average loss:2.28473\n",
      "epoch:1420, Average loss:2.24665\n",
      "epoch:1421, Average loss:2.18096\n",
      "epoch:1422, Average loss:2.263513\n",
      "epoch:1423, Average loss:2.217846\n",
      "epoch:1424, Average loss:2.238223\n",
      "epoch:1425, Average loss:2.237859\n",
      "epoch:1426, Average loss:2.285351\n",
      "epoch:1427, Average loss:2.253241\n",
      "epoch:1428, Average loss:2.267772\n",
      "epoch:1429, Average loss:2.254772\n",
      "epoch:1430, Average loss:2.201391\n",
      "epoch:1431, Average loss:2.236491\n",
      "epoch:1432, Average loss:2.209967\n",
      "epoch:1433, Average loss:2.231556\n",
      "epoch:1434, Average loss:2.272481\n",
      "epoch:1435, Average loss:2.238703\n",
      "epoch:1436, Average loss:2.263848\n",
      "epoch:1437, Average loss:2.23924\n",
      "epoch:1438, Average loss:2.278513\n",
      "epoch:1439, Average loss:2.245403\n",
      "epoch:1440, Average loss:2.192311\n",
      "epoch:1441, Average loss:2.244222\n",
      "epoch:1442, Average loss:2.25892\n",
      "epoch:1443, Average loss:2.237338\n",
      "epoch:1444, Average loss:2.208954\n",
      "epoch:1445, Average loss:2.272725\n",
      "epoch:1446, Average loss:2.193516\n",
      "epoch:1447, Average loss:2.232134\n",
      "epoch:1448, Average loss:2.234696\n",
      "epoch:1449, Average loss:2.209167\n",
      "epoch:1450, Average loss:2.195039\n",
      "epoch:1451, Average loss:2.183142\n",
      "epoch:1452, Average loss:2.225878\n",
      "epoch:1453, Average loss:2.230082\n",
      "epoch:1454, Average loss:2.239241\n",
      "epoch:1455, Average loss:2.235574\n",
      "epoch:1456, Average loss:2.221527\n",
      "epoch:1457, Average loss:2.276386\n",
      "epoch:1458, Average loss:2.213182\n",
      "epoch:1459, Average loss:2.203501\n",
      "epoch:1460, Average loss:2.191283\n",
      "epoch:1461, Average loss:2.205501\n",
      "epoch:1462, Average loss:2.225834\n",
      "epoch:1463, Average loss:2.252211\n",
      "epoch:1464, Average loss:2.226464\n",
      "epoch:1465, Average loss:2.175752\n",
      "epoch:1466, Average loss:2.23703\n",
      "epoch:1467, Average loss:2.185\n",
      "epoch:1468, Average loss:2.238368\n",
      "epoch:1469, Average loss:2.239061\n",
      "epoch:1470, Average loss:2.221928\n",
      "epoch:1471, Average loss:2.218903\n",
      "epoch:1472, Average loss:2.191136\n",
      "epoch:1473, Average loss:2.196416\n",
      "epoch:1474, Average loss:2.243869\n",
      "epoch:1475, Average loss:2.263996\n",
      "epoch:1476, Average loss:2.244224\n",
      "epoch:1477, Average loss:2.213156\n",
      "epoch:1478, Average loss:2.211537\n",
      "epoch:1479, Average loss:2.243888\n",
      "epoch:1480, Average loss:2.169611\n",
      "epoch:1481, Average loss:2.194918\n",
      "epoch:1482, Average loss:2.222908\n",
      "epoch:1483, Average loss:2.210578\n",
      "epoch:1484, Average loss:2.212452\n",
      "epoch:1485, Average loss:2.227765\n",
      "epoch:1486, Average loss:2.216441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1487, Average loss:2.195945\n",
      "epoch:1488, Average loss:2.260262\n",
      "epoch:1489, Average loss:2.173374\n",
      "epoch:1490, Average loss:2.309198\n",
      "epoch:1491, Average loss:2.198625\n",
      "epoch:1492, Average loss:2.191401\n",
      "epoch:1493, Average loss:2.197743\n",
      "epoch:1494, Average loss:2.18977\n",
      "epoch:1495, Average loss:2.229483\n",
      "epoch:1496, Average loss:2.223991\n",
      "epoch:1497, Average loss:2.189849\n",
      "epoch:1498, Average loss:2.16778\n",
      "epoch:1499, Average loss:2.220552\n",
      "epoch:1500, Average loss:2.222478\n",
      "epoch:1501, Average loss:2.177549\n",
      "epoch:1502, Average loss:2.135622\n",
      "epoch:1503, Average loss:2.192788\n",
      "epoch:1504, Average loss:2.167783\n",
      "epoch:1505, Average loss:2.246369\n",
      "epoch:1506, Average loss:2.207463\n",
      "epoch:1507, Average loss:2.214142\n",
      "epoch:1508, Average loss:2.226386\n",
      "epoch:1509, Average loss:2.279199\n",
      "epoch:1510, Average loss:2.181338\n",
      "epoch:1511, Average loss:2.201043\n",
      "epoch:1512, Average loss:2.2031\n",
      "epoch:1513, Average loss:2.195708\n",
      "epoch:1514, Average loss:2.199139\n",
      "epoch:1515, Average loss:2.173961\n",
      "epoch:1516, Average loss:2.236255\n",
      "epoch:1517, Average loss:2.160955\n",
      "epoch:1518, Average loss:2.205587\n",
      "epoch:1519, Average loss:2.136522\n",
      "epoch:1520, Average loss:2.189575\n",
      "epoch:1521, Average loss:2.159708\n",
      "epoch:1522, Average loss:2.208546\n",
      "epoch:1523, Average loss:2.177295\n",
      "epoch:1524, Average loss:2.199602\n",
      "epoch:1525, Average loss:2.197959\n",
      "epoch:1526, Average loss:2.174473\n",
      "epoch:1527, Average loss:2.172171\n",
      "epoch:1528, Average loss:2.171186\n",
      "epoch:1529, Average loss:2.158123\n",
      "epoch:1530, Average loss:2.128696\n",
      "epoch:1531, Average loss:2.192419\n",
      "epoch:1532, Average loss:2.130319\n",
      "epoch:1533, Average loss:2.191929\n",
      "epoch:1534, Average loss:2.189758\n",
      "epoch:1535, Average loss:2.228946\n",
      "epoch:1536, Average loss:2.176594\n",
      "epoch:1537, Average loss:2.2191\n",
      "epoch:1538, Average loss:2.155705\n",
      "epoch:1539, Average loss:2.168943\n",
      "epoch:1540, Average loss:2.182831\n",
      "epoch:1541, Average loss:2.197112\n",
      "epoch:1542, Average loss:2.147109\n",
      "epoch:1543, Average loss:2.127772\n",
      "epoch:1544, Average loss:2.171329\n",
      "epoch:1545, Average loss:2.17881\n",
      "epoch:1546, Average loss:2.174851\n",
      "epoch:1547, Average loss:2.177966\n",
      "epoch:1548, Average loss:2.187593\n",
      "epoch:1549, Average loss:2.173332\n",
      "epoch:1550, Average loss:2.172266\n",
      "epoch:1551, Average loss:2.186122\n",
      "epoch:1552, Average loss:2.181702\n",
      "epoch:1553, Average loss:2.162403\n",
      "epoch:1554, Average loss:2.16205\n",
      "epoch:1555, Average loss:2.165737\n",
      "epoch:1556, Average loss:2.152944\n",
      "epoch:1557, Average loss:2.114809\n",
      "epoch:1558, Average loss:2.161451\n",
      "epoch:1559, Average loss:2.111502\n",
      "epoch:1560, Average loss:2.124043\n",
      "epoch:1561, Average loss:2.174132\n",
      "epoch:1562, Average loss:2.178794\n",
      "epoch:1563, Average loss:2.136806\n",
      "epoch:1564, Average loss:2.155367\n",
      "epoch:1565, Average loss:2.170206\n",
      "epoch:1566, Average loss:2.069259\n",
      "epoch:1567, Average loss:2.185693\n",
      "epoch:1568, Average loss:2.197549\n",
      "epoch:1569, Average loss:2.182917\n",
      "epoch:1570, Average loss:2.158781\n",
      "epoch:1571, Average loss:2.179782\n",
      "epoch:1572, Average loss:2.157545\n",
      "epoch:1573, Average loss:2.153545\n",
      "epoch:1574, Average loss:2.167136\n",
      "epoch:1575, Average loss:2.122758\n",
      "epoch:1576, Average loss:2.144864\n",
      "epoch:1577, Average loss:2.200461\n",
      "epoch:1578, Average loss:2.167331\n",
      "epoch:1579, Average loss:2.116637\n",
      "epoch:1580, Average loss:2.160429\n",
      "epoch:1581, Average loss:2.158423\n",
      "epoch:1582, Average loss:2.131737\n",
      "epoch:1583, Average loss:2.149203\n",
      "epoch:1584, Average loss:2.091057\n",
      "epoch:1585, Average loss:2.137128\n",
      "epoch:1586, Average loss:2.183235\n",
      "epoch:1587, Average loss:2.165541\n",
      "epoch:1588, Average loss:2.108049\n",
      "epoch:1589, Average loss:2.145078\n",
      "epoch:1590, Average loss:2.141654\n",
      "epoch:1591, Average loss:2.173919\n",
      "epoch:1592, Average loss:2.147978\n",
      "epoch:1593, Average loss:2.182995\n",
      "epoch:1594, Average loss:2.140567\n",
      "epoch:1595, Average loss:2.188032\n",
      "epoch:1596, Average loss:2.136195\n",
      "epoch:1597, Average loss:2.147963\n",
      "epoch:1598, Average loss:2.139888\n",
      "epoch:1599, Average loss:2.150402\n",
      "epoch:1600, Average loss:2.149688\n",
      "epoch:1601, Average loss:2.176571\n",
      "epoch:1602, Average loss:2.113691\n",
      "epoch:1603, Average loss:2.149727\n",
      "epoch:1604, Average loss:2.192688\n",
      "epoch:1605, Average loss:2.169147\n",
      "epoch:1606, Average loss:2.072021\n",
      "epoch:1607, Average loss:2.134752\n",
      "epoch:1608, Average loss:2.098259\n",
      "epoch:1609, Average loss:2.130551\n",
      "epoch:1610, Average loss:2.169096\n",
      "epoch:1611, Average loss:2.19411\n",
      "epoch:1612, Average loss:2.132526\n",
      "epoch:1613, Average loss:2.12477\n",
      "epoch:1614, Average loss:2.145825\n",
      "epoch:1615, Average loss:2.144874\n",
      "epoch:1616, Average loss:2.206322\n",
      "epoch:1617, Average loss:2.100426\n",
      "epoch:1618, Average loss:2.133505\n",
      "epoch:1619, Average loss:2.115873\n",
      "epoch:1620, Average loss:2.191901\n",
      "epoch:1621, Average loss:2.145144\n",
      "epoch:1622, Average loss:2.12813\n",
      "epoch:1623, Average loss:2.124133\n",
      "epoch:1624, Average loss:2.13429\n",
      "epoch:1625, Average loss:2.148329\n",
      "epoch:1626, Average loss:2.150689\n",
      "epoch:1627, Average loss:2.144878\n",
      "epoch:1628, Average loss:2.109035\n",
      "epoch:1629, Average loss:2.066699\n",
      "epoch:1630, Average loss:2.102488\n",
      "epoch:1631, Average loss:2.153145\n",
      "epoch:1632, Average loss:2.098289\n",
      "epoch:1633, Average loss:2.147921\n",
      "epoch:1634, Average loss:2.087922\n",
      "epoch:1635, Average loss:2.145293\n",
      "epoch:1636, Average loss:2.093539\n",
      "epoch:1637, Average loss:2.131821\n",
      "epoch:1638, Average loss:2.116157\n",
      "epoch:1639, Average loss:2.113053\n",
      "epoch:1640, Average loss:2.100847\n",
      "epoch:1641, Average loss:2.174539\n",
      "epoch:1642, Average loss:2.156476\n",
      "epoch:1643, Average loss:2.146577\n",
      "epoch:1644, Average loss:2.11925\n",
      "epoch:1645, Average loss:2.099899\n",
      "epoch:1646, Average loss:2.090062\n",
      "epoch:1647, Average loss:2.114641\n",
      "epoch:1648, Average loss:2.10085\n",
      "epoch:1649, Average loss:2.101367\n",
      "epoch:1650, Average loss:2.091764\n",
      "epoch:1651, Average loss:2.071348\n",
      "epoch:1652, Average loss:2.112285\n",
      "epoch:1653, Average loss:2.128953\n",
      "epoch:1654, Average loss:2.112944\n",
      "epoch:1655, Average loss:2.165898\n",
      "epoch:1656, Average loss:2.123457\n",
      "epoch:1657, Average loss:2.075779\n",
      "epoch:1658, Average loss:2.111141\n",
      "epoch:1659, Average loss:2.080037\n",
      "epoch:1660, Average loss:2.122473\n",
      "epoch:1661, Average loss:2.045356\n",
      "epoch:1662, Average loss:2.127982\n",
      "epoch:1663, Average loss:2.12338\n",
      "epoch:1664, Average loss:2.152844\n",
      "epoch:1665, Average loss:2.111456\n",
      "epoch:1666, Average loss:2.090058\n",
      "epoch:1667, Average loss:2.169814\n",
      "epoch:1668, Average loss:2.09811\n",
      "epoch:1669, Average loss:2.057876\n",
      "epoch:1670, Average loss:2.061911\n",
      "epoch:1671, Average loss:2.08271\n",
      "epoch:1672, Average loss:2.022562\n",
      "epoch:1673, Average loss:2.068536\n",
      "epoch:1674, Average loss:2.128758\n",
      "epoch:1675, Average loss:2.071666\n",
      "epoch:1676, Average loss:2.178634\n",
      "epoch:1677, Average loss:2.089067\n",
      "epoch:1678, Average loss:2.110884\n",
      "epoch:1679, Average loss:2.132601\n",
      "epoch:1680, Average loss:2.081637\n",
      "epoch:1681, Average loss:2.089119\n",
      "epoch:1682, Average loss:2.095609\n",
      "epoch:1683, Average loss:2.103986\n",
      "epoch:1684, Average loss:2.04503\n",
      "epoch:1685, Average loss:2.054032\n",
      "epoch:1686, Average loss:2.124038\n",
      "epoch:1687, Average loss:2.090873\n",
      "epoch:1688, Average loss:2.100147\n",
      "epoch:1689, Average loss:2.072116\n",
      "epoch:1690, Average loss:2.123511\n",
      "epoch:1691, Average loss:2.095774\n",
      "epoch:1692, Average loss:2.118978\n",
      "epoch:1693, Average loss:2.085412\n",
      "epoch:1694, Average loss:2.109038\n",
      "epoch:1695, Average loss:2.066961\n",
      "epoch:1696, Average loss:2.065138\n",
      "epoch:1697, Average loss:2.079403\n",
      "epoch:1698, Average loss:2.113599\n",
      "epoch:1699, Average loss:2.09454\n",
      "epoch:1700, Average loss:2.132372\n",
      "epoch:1701, Average loss:2.128368\n",
      "epoch:1702, Average loss:2.115249\n",
      "epoch:1703, Average loss:2.080846\n",
      "epoch:1704, Average loss:2.138088\n",
      "epoch:1705, Average loss:2.089819\n",
      "epoch:1706, Average loss:2.07665\n",
      "epoch:1707, Average loss:2.101608\n",
      "epoch:1708, Average loss:2.125228\n",
      "epoch:1709, Average loss:2.081702\n",
      "epoch:1710, Average loss:2.075364\n",
      "epoch:1711, Average loss:2.079673\n",
      "epoch:1712, Average loss:2.111141\n",
      "epoch:1713, Average loss:2.141453\n",
      "epoch:1714, Average loss:2.081109\n",
      "epoch:1715, Average loss:2.076496\n",
      "epoch:1716, Average loss:2.062779\n",
      "epoch:1717, Average loss:2.120076\n",
      "epoch:1718, Average loss:2.078384\n",
      "epoch:1719, Average loss:2.110968\n",
      "epoch:1720, Average loss:2.062555\n",
      "epoch:1721, Average loss:2.085562\n",
      "epoch:1722, Average loss:2.078122\n",
      "epoch:1723, Average loss:2.087994\n",
      "epoch:1724, Average loss:2.100164\n",
      "epoch:1725, Average loss:2.102318\n",
      "epoch:1726, Average loss:2.049226\n",
      "epoch:1727, Average loss:2.061509\n",
      "epoch:1728, Average loss:2.090836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1729, Average loss:2.043221\n",
      "epoch:1730, Average loss:2.114908\n",
      "epoch:1731, Average loss:2.049584\n",
      "epoch:1732, Average loss:2.097385\n",
      "epoch:1733, Average loss:2.064478\n",
      "epoch:1734, Average loss:2.04194\n",
      "epoch:1735, Average loss:2.025021\n",
      "epoch:1736, Average loss:2.114099\n",
      "epoch:1737, Average loss:2.100654\n",
      "epoch:1738, Average loss:2.067178\n",
      "epoch:1739, Average loss:2.062324\n",
      "epoch:1740, Average loss:2.094278\n",
      "epoch:1741, Average loss:2.06511\n",
      "epoch:1742, Average loss:2.04076\n",
      "epoch:1743, Average loss:2.061028\n",
      "epoch:1744, Average loss:2.089282\n",
      "epoch:1745, Average loss:2.039481\n",
      "epoch:1746, Average loss:2.040233\n",
      "epoch:1747, Average loss:2.011883\n",
      "epoch:1748, Average loss:2.051713\n",
      "epoch:1749, Average loss:2.086383\n",
      "epoch:1750, Average loss:2.078602\n",
      "epoch:1751, Average loss:2.022176\n",
      "epoch:1752, Average loss:2.079159\n",
      "epoch:1753, Average loss:2.024892\n",
      "epoch:1754, Average loss:2.046606\n",
      "epoch:1755, Average loss:2.125515\n",
      "epoch:1756, Average loss:2.08002\n",
      "epoch:1757, Average loss:2.123665\n",
      "epoch:1758, Average loss:2.02687\n",
      "epoch:1759, Average loss:2.100841\n",
      "epoch:1760, Average loss:2.024262\n",
      "epoch:1761, Average loss:2.049422\n",
      "epoch:1762, Average loss:2.071783\n",
      "epoch:1763, Average loss:2.014942\n",
      "epoch:1764, Average loss:2.017661\n",
      "epoch:1765, Average loss:2.020553\n",
      "epoch:1766, Average loss:2.017427\n",
      "epoch:1767, Average loss:2.040577\n",
      "epoch:1768, Average loss:2.088727\n",
      "epoch:1769, Average loss:2.077966\n",
      "epoch:1770, Average loss:2.056258\n",
      "epoch:1771, Average loss:2.06248\n",
      "epoch:1772, Average loss:2.081669\n",
      "epoch:1773, Average loss:2.015713\n",
      "epoch:1774, Average loss:2.074842\n",
      "epoch:1775, Average loss:2.142381\n",
      "epoch:1776, Average loss:2.085407\n",
      "epoch:1777, Average loss:2.063369\n",
      "epoch:1778, Average loss:2.050857\n",
      "epoch:1779, Average loss:2.028769\n",
      "epoch:1780, Average loss:2.117338\n",
      "epoch:1781, Average loss:2.033383\n",
      "epoch:1782, Average loss:1.998912\n",
      "epoch:1783, Average loss:2.065565\n",
      "epoch:1784, Average loss:2.047824\n",
      "epoch:1785, Average loss:2.046535\n",
      "epoch:1786, Average loss:2.099452\n",
      "epoch:1787, Average loss:2.073457\n",
      "epoch:1788, Average loss:2.078293\n",
      "epoch:1789, Average loss:2.08112\n",
      "epoch:1790, Average loss:2.074141\n",
      "epoch:1791, Average loss:2.027607\n",
      "epoch:1792, Average loss:2.043382\n",
      "epoch:1793, Average loss:2.079539\n",
      "epoch:1794, Average loss:2.000954\n",
      "epoch:1795, Average loss:2.037861\n",
      "epoch:1796, Average loss:2.070632\n",
      "epoch:1797, Average loss:1.998299\n",
      "epoch:1798, Average loss:2.00922\n",
      "epoch:1799, Average loss:2.010169\n",
      "epoch:1800, Average loss:2.00383\n",
      "epoch:1801, Average loss:2.022284\n",
      "epoch:1802, Average loss:2.053436\n",
      "epoch:1803, Average loss:2.023812\n",
      "epoch:1804, Average loss:2.042726\n",
      "epoch:1805, Average loss:2.085914\n",
      "epoch:1806, Average loss:2.009659\n",
      "epoch:1807, Average loss:1.982941\n",
      "epoch:1808, Average loss:2.025149\n",
      "epoch:1809, Average loss:2.085234\n",
      "epoch:1810, Average loss:2.071411\n",
      "epoch:1811, Average loss:1.992758\n",
      "epoch:1812, Average loss:2.081532\n",
      "epoch:1813, Average loss:2.02111\n",
      "epoch:1814, Average loss:2.024141\n",
      "epoch:1815, Average loss:2.041253\n",
      "epoch:1816, Average loss:2.006796\n",
      "epoch:1817, Average loss:2.006921\n",
      "epoch:1818, Average loss:2.008086\n",
      "epoch:1819, Average loss:2.032247\n",
      "epoch:1820, Average loss:2.040604\n",
      "epoch:1821, Average loss:2.002619\n",
      "epoch:1822, Average loss:2.025492\n",
      "epoch:1823, Average loss:2.053752\n",
      "epoch:1824, Average loss:2.012267\n",
      "epoch:1825, Average loss:2.099385\n",
      "epoch:1826, Average loss:2.017543\n",
      "epoch:1827, Average loss:2.001476\n",
      "epoch:1828, Average loss:2.071036\n",
      "epoch:1829, Average loss:2.021564\n",
      "epoch:1830, Average loss:2.00049\n",
      "epoch:1831, Average loss:2.025781\n",
      "epoch:1832, Average loss:2.017498\n",
      "epoch:1833, Average loss:2.043362\n",
      "epoch:1834, Average loss:2.027982\n",
      "epoch:1835, Average loss:2.035353\n",
      "epoch:1836, Average loss:1.982004\n",
      "epoch:1837, Average loss:1.990686\n",
      "epoch:1838, Average loss:2.023764\n",
      "epoch:1839, Average loss:2.004298\n",
      "epoch:1840, Average loss:2.052136\n",
      "epoch:1841, Average loss:1.987113\n",
      "epoch:1842, Average loss:2.028073\n",
      "epoch:1843, Average loss:2.027194\n",
      "epoch:1844, Average loss:2.01055\n",
      "epoch:1845, Average loss:2.022009\n",
      "epoch:1846, Average loss:2.008579\n",
      "epoch:1847, Average loss:2.016504\n",
      "epoch:1848, Average loss:2.004413\n",
      "epoch:1849, Average loss:1.972984\n",
      "epoch:1850, Average loss:1.973612\n",
      "epoch:1851, Average loss:2.100396\n",
      "epoch:1852, Average loss:2.025173\n",
      "epoch:1853, Average loss:1.985707\n",
      "epoch:1854, Average loss:2.014154\n",
      "epoch:1855, Average loss:2.006976\n",
      "epoch:1856, Average loss:2.028784\n",
      "epoch:1857, Average loss:2.033289\n",
      "epoch:1858, Average loss:2.032246\n",
      "epoch:1859, Average loss:1.951018\n",
      "epoch:1860, Average loss:2.043883\n",
      "epoch:1861, Average loss:1.99789\n",
      "epoch:1862, Average loss:1.988367\n",
      "epoch:1863, Average loss:2.026552\n",
      "epoch:1864, Average loss:2.002909\n",
      "epoch:1865, Average loss:1.989846\n",
      "epoch:1866, Average loss:2.027587\n",
      "epoch:1867, Average loss:2.060607\n",
      "epoch:1868, Average loss:2.002808\n",
      "epoch:1869, Average loss:1.94771\n",
      "epoch:1870, Average loss:1.973689\n",
      "epoch:1871, Average loss:2.04807\n",
      "epoch:1872, Average loss:1.99413\n",
      "epoch:1873, Average loss:1.990549\n",
      "epoch:1874, Average loss:1.997156\n",
      "epoch:1875, Average loss:2.057318\n",
      "epoch:1876, Average loss:1.980088\n",
      "epoch:1877, Average loss:2.020915\n",
      "epoch:1878, Average loss:2.019293\n",
      "epoch:1879, Average loss:1.97942\n",
      "epoch:1880, Average loss:1.979828\n",
      "epoch:1881, Average loss:2.01318\n",
      "epoch:1882, Average loss:2.003281\n",
      "epoch:1883, Average loss:1.960693\n",
      "epoch:1884, Average loss:1.983633\n",
      "epoch:1885, Average loss:2.041063\n",
      "epoch:1886, Average loss:2.048333\n",
      "epoch:1887, Average loss:1.980361\n",
      "epoch:1888, Average loss:1.951112\n",
      "epoch:1889, Average loss:1.998976\n",
      "epoch:1890, Average loss:1.996712\n",
      "epoch:1891, Average loss:2.013417\n",
      "epoch:1892, Average loss:2.02202\n",
      "epoch:1893, Average loss:2.008094\n",
      "epoch:1894, Average loss:1.983774\n",
      "epoch:1895, Average loss:2.020064\n",
      "epoch:1896, Average loss:1.967591\n",
      "epoch:1897, Average loss:2.020048\n",
      "epoch:1898, Average loss:1.986337\n",
      "epoch:1899, Average loss:1.994988\n",
      "epoch:1900, Average loss:2.030679\n",
      "epoch:1901, Average loss:1.974071\n",
      "epoch:1902, Average loss:1.989162\n",
      "epoch:1903, Average loss:1.992781\n",
      "epoch:1904, Average loss:2.015523\n",
      "epoch:1905, Average loss:1.970828\n",
      "epoch:1906, Average loss:2.010734\n",
      "epoch:1907, Average loss:1.998303\n",
      "epoch:1908, Average loss:2.009369\n",
      "epoch:1909, Average loss:1.985011\n",
      "epoch:1910, Average loss:1.969641\n",
      "epoch:1911, Average loss:1.998196\n",
      "epoch:1912, Average loss:2.007192\n",
      "epoch:1913, Average loss:2.046262\n",
      "epoch:1914, Average loss:2.010216\n",
      "epoch:1915, Average loss:1.941457\n",
      "epoch:1916, Average loss:1.970518\n",
      "epoch:1917, Average loss:2.008317\n",
      "epoch:1918, Average loss:1.981524\n",
      "epoch:1919, Average loss:2.013014\n",
      "epoch:1920, Average loss:2.016984\n",
      "epoch:1921, Average loss:2.020694\n",
      "epoch:1922, Average loss:1.994916\n",
      "epoch:1923, Average loss:2.017276\n",
      "epoch:1924, Average loss:2.050351\n",
      "epoch:1925, Average loss:1.986447\n",
      "epoch:1926, Average loss:1.991846\n",
      "epoch:1927, Average loss:1.975031\n",
      "epoch:1928, Average loss:1.989838\n",
      "epoch:1929, Average loss:1.969939\n",
      "epoch:1930, Average loss:1.978364\n",
      "epoch:1931, Average loss:1.974366\n",
      "epoch:1932, Average loss:1.926632\n",
      "epoch:1933, Average loss:2.032316\n",
      "epoch:1934, Average loss:2.013466\n",
      "epoch:1935, Average loss:1.959272\n",
      "epoch:1936, Average loss:2.028909\n",
      "epoch:1937, Average loss:1.986586\n",
      "epoch:1938, Average loss:1.976226\n",
      "epoch:1939, Average loss:1.97502\n",
      "epoch:1940, Average loss:1.960548\n",
      "epoch:1941, Average loss:1.94347\n",
      "epoch:1942, Average loss:1.990113\n",
      "epoch:1943, Average loss:1.968678\n",
      "epoch:1944, Average loss:1.945152\n",
      "epoch:1945, Average loss:1.943182\n",
      "epoch:1946, Average loss:1.972868\n",
      "epoch:1947, Average loss:2.022473\n",
      "epoch:1948, Average loss:2.003005\n",
      "epoch:1949, Average loss:1.988536\n",
      "epoch:1950, Average loss:1.953763\n",
      "epoch:1951, Average loss:1.980946\n",
      "epoch:1952, Average loss:1.992404\n",
      "epoch:1953, Average loss:2.001923\n",
      "epoch:1954, Average loss:1.979479\n",
      "epoch:1955, Average loss:1.990361\n",
      "epoch:1956, Average loss:2.003935\n",
      "epoch:1957, Average loss:1.973968\n",
      "epoch:1958, Average loss:1.954246\n",
      "epoch:1959, Average loss:1.946232\n",
      "epoch:1960, Average loss:1.95143\n",
      "epoch:1961, Average loss:1.957946\n",
      "epoch:1962, Average loss:1.902169\n",
      "epoch:1963, Average loss:1.944922\n",
      "epoch:1964, Average loss:1.951023\n",
      "epoch:1965, Average loss:1.964191\n",
      "epoch:1966, Average loss:1.975947\n",
      "epoch:1967, Average loss:1.983092\n",
      "epoch:1968, Average loss:1.975292\n",
      "epoch:1969, Average loss:1.965476\n",
      "epoch:1970, Average loss:1.967179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1971, Average loss:1.948637\n",
      "epoch:1972, Average loss:1.968299\n",
      "epoch:1973, Average loss:1.926799\n",
      "epoch:1974, Average loss:1.910455\n",
      "epoch:1975, Average loss:1.933763\n",
      "epoch:1976, Average loss:1.919452\n",
      "epoch:1977, Average loss:1.964805\n",
      "epoch:1978, Average loss:1.995504\n",
      "epoch:1979, Average loss:2.008202\n",
      "epoch:1980, Average loss:1.97159\n",
      "epoch:1981, Average loss:1.956993\n",
      "epoch:1982, Average loss:1.915902\n",
      "epoch:1983, Average loss:2.054428\n",
      "epoch:1984, Average loss:1.975965\n",
      "epoch:1985, Average loss:1.946959\n",
      "epoch:1986, Average loss:1.950882\n",
      "epoch:1987, Average loss:1.950213\n",
      "epoch:1988, Average loss:1.919876\n",
      "epoch:1989, Average loss:1.968653\n",
      "epoch:1990, Average loss:1.929878\n",
      "epoch:1991, Average loss:1.914705\n",
      "epoch:1992, Average loss:1.935561\n",
      "epoch:1993, Average loss:2.000151\n",
      "epoch:1994, Average loss:1.963167\n",
      "epoch:1995, Average loss:1.912069\n",
      "epoch:1996, Average loss:1.933824\n",
      "epoch:1997, Average loss:1.966587\n",
      "epoch:1998, Average loss:1.986151\n",
      "epoch:1999, Average loss:1.949383\n",
      "epoch:2000, Average loss:2.022199\n",
      "epoch:2001, Average loss:1.956549\n",
      "epoch:2002, Average loss:1.973157\n",
      "epoch:2003, Average loss:1.90949\n",
      "epoch:2004, Average loss:1.978717\n",
      "epoch:2005, Average loss:1.926262\n",
      "epoch:2006, Average loss:1.895139\n",
      "epoch:2007, Average loss:1.959704\n",
      "epoch:2008, Average loss:1.958951\n",
      "epoch:2009, Average loss:1.916597\n",
      "epoch:2010, Average loss:1.988322\n",
      "epoch:2011, Average loss:1.913821\n",
      "epoch:2012, Average loss:1.92601\n",
      "epoch:2013, Average loss:1.925288\n",
      "epoch:2014, Average loss:1.960611\n",
      "epoch:2015, Average loss:1.962256\n",
      "epoch:2016, Average loss:1.937739\n",
      "epoch:2017, Average loss:1.952508\n",
      "epoch:2018, Average loss:1.961094\n",
      "epoch:2019, Average loss:2.007644\n",
      "epoch:2020, Average loss:1.93375\n",
      "epoch:2021, Average loss:1.896233\n",
      "epoch:2022, Average loss:2.007438\n",
      "epoch:2023, Average loss:1.972192\n",
      "epoch:2024, Average loss:1.929982\n",
      "epoch:2025, Average loss:1.938833\n",
      "epoch:2026, Average loss:1.923365\n",
      "epoch:2027, Average loss:1.919683\n",
      "epoch:2028, Average loss:1.899117\n",
      "epoch:2029, Average loss:1.901674\n",
      "epoch:2030, Average loss:1.945479\n",
      "epoch:2031, Average loss:1.91893\n",
      "epoch:2032, Average loss:1.914692\n",
      "epoch:2033, Average loss:1.948436\n",
      "epoch:2034, Average loss:1.966246\n",
      "epoch:2035, Average loss:1.952609\n",
      "epoch:2036, Average loss:1.90363\n",
      "epoch:2037, Average loss:1.988877\n",
      "epoch:2038, Average loss:1.89809\n",
      "epoch:2039, Average loss:1.953197\n",
      "epoch:2040, Average loss:1.912778\n",
      "epoch:2041, Average loss:1.94642\n",
      "epoch:2042, Average loss:1.893832\n",
      "epoch:2043, Average loss:1.923475\n",
      "epoch:2044, Average loss:1.897708\n",
      "epoch:2045, Average loss:1.946431\n",
      "epoch:2046, Average loss:1.965631\n",
      "epoch:2047, Average loss:1.909796\n",
      "epoch:2048, Average loss:1.84861\n",
      "epoch:2049, Average loss:2.022169\n",
      "epoch:2050, Average loss:1.945587\n",
      "epoch:2051, Average loss:2.006565\n",
      "epoch:2052, Average loss:1.944364\n",
      "epoch:2053, Average loss:1.928321\n",
      "epoch:2054, Average loss:1.913158\n",
      "epoch:2055, Average loss:1.969055\n",
      "epoch:2056, Average loss:1.953337\n",
      "epoch:2057, Average loss:1.874038\n",
      "epoch:2058, Average loss:1.905376\n",
      "epoch:2059, Average loss:1.91351\n",
      "epoch:2060, Average loss:1.909365\n",
      "epoch:2061, Average loss:1.907822\n",
      "epoch:2062, Average loss:1.924496\n",
      "epoch:2063, Average loss:1.90577\n",
      "epoch:2064, Average loss:1.94738\n",
      "epoch:2065, Average loss:1.939283\n",
      "epoch:2066, Average loss:1.895173\n",
      "epoch:2067, Average loss:1.936784\n",
      "epoch:2068, Average loss:1.91871\n",
      "epoch:2069, Average loss:1.902978\n",
      "epoch:2070, Average loss:1.878711\n",
      "epoch:2071, Average loss:1.969241\n",
      "epoch:2072, Average loss:1.943018\n",
      "epoch:2073, Average loss:1.90108\n",
      "epoch:2074, Average loss:1.920499\n",
      "epoch:2075, Average loss:1.90169\n",
      "epoch:2076, Average loss:1.911537\n",
      "epoch:2077, Average loss:1.888181\n",
      "epoch:2078, Average loss:1.900089\n",
      "epoch:2079, Average loss:1.895009\n",
      "epoch:2080, Average loss:1.912188\n",
      "epoch:2081, Average loss:1.885071\n",
      "epoch:2082, Average loss:1.955473\n",
      "epoch:2083, Average loss:1.960296\n",
      "epoch:2084, Average loss:1.938024\n",
      "epoch:2085, Average loss:1.960474\n",
      "epoch:2086, Average loss:1.913602\n",
      "epoch:2087, Average loss:1.910541\n",
      "epoch:2088, Average loss:1.931257\n",
      "epoch:2089, Average loss:1.868322\n",
      "epoch:2090, Average loss:1.930525\n",
      "epoch:2091, Average loss:1.884105\n",
      "epoch:2092, Average loss:1.918803\n",
      "epoch:2093, Average loss:1.930581\n",
      "epoch:2094, Average loss:1.951748\n",
      "epoch:2095, Average loss:1.890569\n",
      "epoch:2096, Average loss:1.940247\n",
      "epoch:2097, Average loss:1.860723\n",
      "epoch:2098, Average loss:1.942573\n",
      "epoch:2099, Average loss:1.892707\n",
      "epoch:2100, Average loss:1.902325\n",
      "epoch:2101, Average loss:1.86971\n",
      "epoch:2102, Average loss:1.906882\n",
      "epoch:2103, Average loss:1.910255\n",
      "epoch:2104, Average loss:1.898455\n",
      "epoch:2105, Average loss:1.882437\n",
      "epoch:2106, Average loss:1.864017\n",
      "epoch:2107, Average loss:1.896988\n",
      "epoch:2108, Average loss:1.895319\n",
      "epoch:2109, Average loss:1.907217\n",
      "epoch:2110, Average loss:1.883987\n",
      "epoch:2111, Average loss:1.893485\n",
      "epoch:2112, Average loss:1.919434\n",
      "epoch:2113, Average loss:1.960092\n",
      "epoch:2114, Average loss:1.909763\n",
      "epoch:2115, Average loss:1.846787\n",
      "epoch:2116, Average loss:1.86481\n",
      "epoch:2117, Average loss:1.949926\n",
      "epoch:2118, Average loss:1.885006\n",
      "epoch:2119, Average loss:1.912438\n",
      "epoch:2120, Average loss:1.927288\n",
      "epoch:2121, Average loss:1.906868\n",
      "epoch:2122, Average loss:1.889128\n",
      "epoch:2123, Average loss:1.9736\n",
      "epoch:2124, Average loss:1.87998\n",
      "epoch:2125, Average loss:1.884421\n",
      "epoch:2126, Average loss:1.879949\n",
      "epoch:2127, Average loss:1.902607\n",
      "epoch:2128, Average loss:1.926498\n",
      "epoch:2129, Average loss:1.893688\n",
      "epoch:2130, Average loss:1.939596\n",
      "epoch:2131, Average loss:1.852869\n",
      "epoch:2132, Average loss:1.919905\n",
      "epoch:2133, Average loss:1.909163\n",
      "epoch:2134, Average loss:1.897713\n",
      "epoch:2135, Average loss:1.896731\n",
      "epoch:2136, Average loss:1.89214\n",
      "epoch:2137, Average loss:1.849909\n",
      "epoch:2138, Average loss:1.94534\n",
      "epoch:2139, Average loss:1.901571\n",
      "epoch:2140, Average loss:1.881649\n",
      "epoch:2141, Average loss:1.869835\n",
      "epoch:2142, Average loss:1.94771\n",
      "epoch:2143, Average loss:1.903836\n",
      "epoch:2144, Average loss:1.87974\n",
      "epoch:2145, Average loss:1.911428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-82f74b10f960>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mlist_batch_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss_record = []\n",
    "list_batch_labels = []\n",
    "list_batch_inputs = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #Shuffle the list of nodes at the start of each epoch\n",
    "        random.shuffle(list_of_nodes)\n",
    "        random_walks = []\n",
    "        \n",
    "        for node in list_of_nodes:\n",
    "            #Step through each node and conduct a random walk about it of length max_step\n",
    "            path = randomWalk(web_graph, node, 0, max_step, [node])\n",
    "            \n",
    "            path = [domain_map[i] for i in path]\n",
    "            random_walks.append(path)\n",
    "        \n",
    "        data_windows = np.array(random_walks)\n",
    "                \n",
    "        target = data_windows[:,window_size]\n",
    "\n",
    "        left_window = data_windows[:,:window_size]\n",
    "\n",
    "        right_window = data_windows[:,window_size+1:]\n",
    "\n",
    "        context_window = np.concatenate([left_window, right_window], axis=1)\n",
    "            \n",
    "        for step in range(num_steps):\n",
    "\n",
    "            batch_data = generateBatch(batch_size, num_skips, context_window, target, step)\n",
    "            batch_inputs = [row[0] for row in batch_data]\n",
    "            batch_labels = [row[1] for row in batch_data]\n",
    "           \n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "            list_batch_labels.append([batch_labels])\n",
    "            list_batch_inputs.append([batch_inputs])\n",
    "            \n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            average_loss += loss_val\n",
    "         \n",
    "        if epoch%1==0: \n",
    "            \n",
    "            avg_loss_record.append(float(average_loss)/num_steps)\n",
    "            print('epoch:%d, Average loss:%.7g' % (epoch, float(average_loss)/num_steps))\n",
    "        \n",
    "        average_loss = 0\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAF3CAYAAAB5QUrKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1fwJ4stiREFy034AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1abf0ac9f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_loss = np.array(avg_loss_record)\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot([i for \n",
    "          i in range(len(list_loss))], list_loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "#Create cosine similarity matrix\n",
    "\n",
    "mult_vector = tf.matmul(vector, vector, transpose_b=True)\n",
    "sim_matrix = tf.acos(mult_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    matrices = sess.run([sim_matrix, mult_vector] , feed_dict={vector:final_embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = matrices[0]\n",
    "\n",
    "np.fill_diagonal(sim_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_embeddings, open('embeddings_test_domain_graph_undirected.pkl', 'wb'))\n",
    "pickle.dump(sim_matrix, open('cosine_matrix_test_domain_graph_undirected.pkl', 'wb'))\n",
    "pickle.dump(domain_inv_map, open('domain_inv_map.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
